{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.SwinV2Model.train_cv import train_cv as train_SwinV2Model\n",
    "from models.SwinV2Model.predict import predict as predict_SwinV2Model\n",
    "\n",
    "from utils.show_graph import plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2023-09-07 23:33:55 - Fold: 1/5\n",
      "[DEBUG] 2023-09-07 23:33:55 - --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\.venv\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "[DEBUG] 2023-09-07 23:33:57 - Using device: cuda(NVIDIA GeForce RTX 3080)\n",
      "[DEBUG] 2023-09-07 23:33:57 - Starting training on cuda at 2023-09-07 23:33:57\n",
      "[DEBUG] 2023-09-07 23:33:57 - Model parameters:\n",
      "[DEBUG] 2023-09-07 23:33:57 - patch_embed.proj.weight: torch.Size([192, 3, 4, 4])\n",
      "[DEBUG] 2023-09-07 23:33:57 - patch_embed.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - patch_embed.norm.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - patch_embed.norm.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.0.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.0.blocks.1.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.downsample.reduction.weight: torch.Size([384, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.downsample.norm.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.downsample.norm.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.0.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.1.blocks.1.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.downsample.reduction.weight: torch.Size([768, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.downsample.norm.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.downsample.norm.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.0.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.1.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.2.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.3.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.4.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.5.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.6.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.7.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.8.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.9.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.10.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.11.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.12.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.13.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.14.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.15.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.16.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.2.blocks.17.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.downsample.reduction.weight: torch.Size([1536, 3072])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.downsample.norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.downsample.norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.0.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:57 - layers.3.blocks.1.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - layers.3.blocks.1.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - head.fc.weight: torch.Size([2, 1536])\n",
      "[DEBUG] 2023-09-07 23:33:58 - head.fc.bias: torch.Size([2])\n",
      "[DEBUG] 2023-09-07 23:33:58 - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "[DEBUG] 2023-09-07 23:33:58 - Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001B3EFA020D0>\n",
      "[DEBUG] 2023-09-07 23:33:58 - Loss function: CrossEntropyLoss()\n",
      "[DEBUG] 2023-09-07 23:33:58 - Epoch: 1 / 20\n",
      "[DEBUG] 2023-09-07 23:33:58 - --------------------------\n",
      "Epoch 1/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-07 23:40:13 - train Loss: 0.5196 AUC: 0.8161\n",
      "Epoch 1/20: 100%|██████████| 436/436 [00:28<00:00, 15.27it/s]\n",
      "[DEBUG] 2023-09-07 23:40:42 - val Loss: 0.3863 AUC: 0.8974\n",
      "[DEBUG] 2023-09-07 23:40:43 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_1_auc_0.8974.pth\n",
      "[DEBUG] 2023-09-07 23:40:43 - Epoch: 2 / 20\n",
      "[DEBUG] 2023-09-07 23:40:43 - --------------------------\n",
      "Epoch 2/20: 100%|██████████| 1740/1740 [06:10<00:00,  4.69it/s]\n",
      "[DEBUG] 2023-09-07 23:46:54 - train Loss: 0.3152 AUC: 0.9391\n",
      "Epoch 2/20: 100%|██████████| 436/436 [00:28<00:00, 15.23it/s]\n",
      "[DEBUG] 2023-09-07 23:47:22 - val Loss: 0.2966 AUC: 0.9440\n",
      "[DEBUG] 2023-09-07 23:47:23 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_2_auc_0.9440.pth\n",
      "[DEBUG] 2023-09-07 23:47:23 - Epoch: 3 / 20\n",
      "[DEBUG] 2023-09-07 23:47:23 - --------------------------\n",
      "Epoch 3/20: 100%|██████████| 1740/1740 [06:11<00:00,  4.69it/s]\n",
      "[DEBUG] 2023-09-07 23:53:34 - train Loss: 0.1493 AUC: 0.9866\n",
      "Epoch 3/20: 100%|██████████| 436/436 [00:28<00:00, 15.11it/s]\n",
      "[DEBUG] 2023-09-07 23:54:03 - val Loss: 0.2714 AUC: 0.9613\n",
      "[DEBUG] 2023-09-07 23:54:04 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_3_auc_0.9613.pth\n",
      "[DEBUG] 2023-09-07 23:54:04 - Epoch: 4 / 20\n",
      "[DEBUG] 2023-09-07 23:54:04 - --------------------------\n",
      "Epoch 4/20: 100%|██████████| 1740/1740 [06:10<00:00,  4.69it/s]\n",
      "[DEBUG] 2023-09-08 00:00:15 - train Loss: 0.0827 AUC: 0.9957\n",
      "Epoch 4/20: 100%|██████████| 436/436 [00:28<00:00, 15.23it/s]\n",
      "[DEBUG] 2023-09-08 00:00:43 - val Loss: 0.2475 AUC: 0.9728\n",
      "[DEBUG] 2023-09-08 00:00:44 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_4_auc_0.9728.pth\n",
      "[DEBUG] 2023-09-08 00:00:44 - Epoch: 5 / 20\n",
      "[DEBUG] 2023-09-08 00:00:44 - --------------------------\n",
      "Epoch 5/20: 100%|██████████| 1740/1740 [06:14<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 00:06:58 - train Loss: 0.0685 AUC: 0.9962\n",
      "Epoch 5/20: 100%|██████████| 436/436 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 00:07:27 - val Loss: 0.2441 AUC: 0.9689\n",
      "[DEBUG] 2023-09-08 00:07:27 - Epoch: 6 / 20\n",
      "[DEBUG] 2023-09-08 00:07:27 - --------------------------\n",
      "Epoch 6/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 00:13:43 - train Loss: 0.0675 AUC: 0.9973\n",
      "Epoch 6/20: 100%|██████████| 436/436 [00:28<00:00, 15.24it/s]\n",
      "[DEBUG] 2023-09-08 00:14:11 - val Loss: 0.2329 AUC: 0.9742\n",
      "[DEBUG] 2023-09-08 00:14:12 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_6_auc_0.9742.pth\n",
      "[DEBUG] 2023-09-08 00:14:12 - Epoch: 7 / 20\n",
      "[DEBUG] 2023-09-08 00:14:12 - --------------------------\n",
      "Epoch 7/20: 100%|██████████| 1740/1740 [06:14<00:00,  4.65it/s]\n",
      "[DEBUG] 2023-09-08 00:20:26 - train Loss: 0.0542 AUC: 0.9974\n",
      "Epoch 7/20: 100%|██████████| 436/436 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 00:20:55 - val Loss: 0.2798 AUC: 0.9653\n",
      "[DEBUG] 2023-09-08 00:20:55 - Epoch: 8 / 20\n",
      "[DEBUG] 2023-09-08 00:20:55 - --------------------------\n",
      "Epoch 8/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 00:27:11 - train Loss: 0.0507 AUC: 0.9979\n",
      "Epoch 8/20: 100%|██████████| 436/436 [00:28<00:00, 15.11it/s]\n",
      "[DEBUG] 2023-09-08 00:27:40 - val Loss: 0.2164 AUC: 0.9795\n",
      "[DEBUG] 2023-09-08 00:27:41 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_8_auc_0.9795.pth\n",
      "[DEBUG] 2023-09-08 00:27:41 - Epoch: 9 / 20\n",
      "[DEBUG] 2023-09-08 00:27:41 - --------------------------\n",
      "Epoch 9/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 00:33:56 - train Loss: 0.0250 AUC: 0.9994\n",
      "Epoch 9/20: 100%|██████████| 436/436 [00:28<00:00, 15.10it/s]\n",
      "[DEBUG] 2023-09-08 00:34:25 - val Loss: 0.3312 AUC: 0.9741\n",
      "[DEBUG] 2023-09-08 00:34:25 - Epoch: 10 / 20\n",
      "[DEBUG] 2023-09-08 00:34:25 - --------------------------\n",
      "Epoch 10/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 00:40:41 - train Loss: 0.0592 AUC: 0.9980\n",
      "Epoch 10/20: 100%|██████████| 436/436 [00:28<00:00, 15.13it/s]\n",
      "[DEBUG] 2023-09-08 00:41:10 - val Loss: 0.2310 AUC: 0.9801\n",
      "[DEBUG] 2023-09-08 00:41:10 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_10_auc_0.9801.pth\n",
      "[DEBUG] 2023-09-08 00:41:10 - Epoch: 11 / 20\n",
      "[DEBUG] 2023-09-08 00:41:10 - --------------------------\n",
      "Epoch 11/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 00:47:26 - train Loss: 0.0320 AUC: 0.9993\n",
      "Epoch 11/20: 100%|██████████| 436/436 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 00:47:55 - val Loss: 0.3627 AUC: 0.9603\n",
      "[DEBUG] 2023-09-08 00:47:55 - Epoch: 12 / 20\n",
      "[DEBUG] 2023-09-08 00:47:55 - --------------------------\n",
      "Epoch 12/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 00:54:11 - train Loss: 0.0198 AUC: 0.9994\n",
      "Epoch 12/20: 100%|██████████| 436/436 [00:28<00:00, 15.09it/s]\n",
      "[DEBUG] 2023-09-08 00:54:40 - val Loss: 0.2562 AUC: 0.9805\n",
      "[DEBUG] 2023-09-08 00:54:40 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_12_auc_0.9805.pth\n",
      "[DEBUG] 2023-09-08 00:54:40 - Epoch: 13 / 20\n",
      "[DEBUG] 2023-09-08 00:54:40 - --------------------------\n",
      "Epoch 13/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 01:00:56 - train Loss: 0.0182 AUC: 0.9998\n",
      "Epoch 13/20: 100%|██████████| 436/436 [00:28<00:00, 15.08it/s]\n",
      "[DEBUG] 2023-09-08 01:01:25 - val Loss: 0.1957 AUC: 0.9851\n",
      "[DEBUG] 2023-09-08 01:01:26 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_13_auc_0.9851.pth\n",
      "[DEBUG] 2023-09-08 01:01:26 - Epoch: 14 / 20\n",
      "[DEBUG] 2023-09-08 01:01:26 - --------------------------\n",
      "Epoch 14/20: 100%|██████████| 1740/1740 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 01:07:41 - train Loss: 0.0560 AUC: 0.9977\n",
      "Epoch 14/20: 100%|██████████| 436/436 [00:28<00:00, 15.19it/s]\n",
      "[DEBUG] 2023-09-08 01:08:10 - val Loss: 0.2427 AUC: 0.9799\n",
      "[DEBUG] 2023-09-08 01:08:10 - Epoch: 15 / 20\n",
      "[DEBUG] 2023-09-08 01:08:10 - --------------------------\n",
      "Epoch 15/20: 100%|██████████| 1740/1740 [06:14<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 01:14:25 - train Loss: 0.0117 AUC: 1.0000\n",
      "Epoch 15/20: 100%|██████████| 436/436 [00:28<00:00, 15.10it/s]\n",
      "[DEBUG] 2023-09-08 01:14:54 - val Loss: 0.2617 AUC: 0.9825\n",
      "[DEBUG] 2023-09-08 01:14:54 - Epoch: 16 / 20\n",
      "[DEBUG] 2023-09-08 01:14:54 - --------------------------\n",
      "Epoch 16/20: 100%|██████████| 1740/1740 [06:11<00:00,  4.68it/s]\n",
      "[DEBUG] 2023-09-08 01:21:06 - train Loss: 0.0339 AUC: 0.9992\n",
      "Epoch 16/20: 100%|██████████| 436/436 [00:28<00:00, 15.22it/s]\n",
      "[DEBUG] 2023-09-08 01:21:35 - val Loss: 0.3433 AUC: 0.9677\n",
      "[DEBUG] 2023-09-08 01:21:35 - Epoch: 17 / 20\n",
      "[DEBUG] 2023-09-08 01:21:35 - --------------------------\n",
      "Epoch 17/20: 100%|██████████| 1740/1740 [06:13<00:00,  4.65it/s]\n",
      "[DEBUG] 2023-09-08 01:27:48 - train Loss: 0.0033 AUC: 1.0000\n",
      "Epoch 17/20: 100%|██████████| 436/436 [00:29<00:00, 14.97it/s]\n",
      "[DEBUG] 2023-09-08 01:28:18 - val Loss: 0.2420 AUC: 0.9850\n",
      "[DEBUG] 2023-09-08 01:28:18 - Epoch: 18 / 20\n",
      "[DEBUG] 2023-09-08 01:28:18 - --------------------------\n",
      "Epoch 18/20: 100%|██████████| 1740/1740 [06:14<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 01:34:32 - train Loss: 0.0437 AUC: 0.9983\n",
      "Epoch 18/20: 100%|██████████| 436/436 [00:28<00:00, 15.40it/s]\n",
      "[DEBUG] 2023-09-08 01:35:01 - val Loss: 0.2340 AUC: 0.9794\n",
      "[DEBUG] 2023-09-08 01:35:01 - Epoch: 19 / 20\n",
      "[DEBUG] 2023-09-08 01:35:01 - --------------------------\n",
      "Epoch 19/20: 100%|██████████| 1740/1740 [06:07<00:00,  4.74it/s]\n",
      "[DEBUG] 2023-09-08 01:41:08 - train Loss: 0.0092 AUC: 1.0000\n",
      "Epoch 19/20: 100%|██████████| 436/436 [00:28<00:00, 15.33it/s]\n",
      "[DEBUG] 2023-09-08 01:41:37 - val Loss: 0.3281 AUC: 0.9754\n",
      "[DEBUG] 2023-09-08 01:41:37 - Epoch: 20 / 20\n",
      "[DEBUG] 2023-09-08 01:41:37 - --------------------------\n",
      "Epoch 20/20: 100%|██████████| 1740/1740 [06:06<00:00,  4.75it/s]\n",
      "[DEBUG] 2023-09-08 01:47:43 - train Loss: 0.0161 AUC: 0.9997\n",
      "Epoch 20/20: 100%|██████████| 436/436 [00:27<00:00, 15.62it/s]\n",
      "[DEBUG] 2023-09-08 01:48:11 - val Loss: 0.2816 AUC: 0.9797\n",
      "[DEBUG] 2023-09-08 01:48:11 - Training complete. Best AUC: 0.9851\n",
      "[DEBUG] 2023-09-08 01:48:11 - Best model path: c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230907233357_epoch_13_auc_0.9851.pth\n",
      "[DEBUG] 2023-09-08 01:48:11 - Fold: 2/5\n",
      "[DEBUG] 2023-09-08 01:48:11 - --------------------\n",
      "[DEBUG] 2023-09-08 01:48:13 - Using device: cuda(NVIDIA GeForce RTX 3080)\n",
      "[DEBUG] 2023-09-08 01:48:13 - Starting training on cuda at 2023-09-08 01:48:13\n",
      "[DEBUG] 2023-09-08 01:48:13 - Model parameters:\n",
      "[DEBUG] 2023-09-08 01:48:13 - patch_embed.proj.weight: torch.Size([192, 3, 4, 4])\n",
      "[DEBUG] 2023-09-08 01:48:13 - patch_embed.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - patch_embed.norm.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - patch_embed.norm.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.0.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.0.blocks.1.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.downsample.reduction.weight: torch.Size([384, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.downsample.norm.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.downsample.norm.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.0.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.1.blocks.1.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.downsample.reduction.weight: torch.Size([768, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.downsample.norm.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.downsample.norm.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.0.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.1.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.2.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.3.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.4.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.5.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.6.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.7.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.8.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.9.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.10.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.11.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.12.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.13.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.14.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.15.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.16.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.2.blocks.17.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.downsample.reduction.weight: torch.Size([1536, 3072])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.downsample.norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.downsample.norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.0.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - layers.3.blocks.1.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - head.fc.weight: torch.Size([2, 1536])\n",
      "[DEBUG] 2023-09-08 01:48:13 - head.fc.bias: torch.Size([2])\n",
      "[DEBUG] 2023-09-08 01:48:13 - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "[DEBUG] 2023-09-08 01:48:13 - Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001B3F2D85A10>\n",
      "[DEBUG] 2023-09-08 01:48:13 - Loss function: CrossEntropyLoss()\n",
      "[DEBUG] 2023-09-08 01:48:13 - Epoch: 1 / 20\n",
      "[DEBUG] 2023-09-08 01:48:13 - --------------------------\n",
      "Epoch 1/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 01:54:28 - train Loss: 0.4937 AUC: 0.8354\n",
      "Epoch 1/20: 100%|██████████| 435/435 [00:28<00:00, 15.23it/s]\n",
      "[DEBUG] 2023-09-08 01:54:57 - val Loss: 0.3128 AUC: 0.9423\n",
      "[DEBUG] 2023-09-08 01:54:58 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_1_auc_0.9423.pth\n",
      "[DEBUG] 2023-09-08 01:54:58 - Epoch: 2 / 20\n",
      "[DEBUG] 2023-09-08 01:54:58 - --------------------------\n",
      "Epoch 2/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:01:15 - train Loss: 0.2885 AUC: 0.9465\n",
      "Epoch 2/20: 100%|██████████| 435/435 [00:28<00:00, 15.19it/s]\n",
      "[DEBUG] 2023-09-08 02:01:44 - val Loss: 0.2519 AUC: 0.9621\n",
      "[DEBUG] 2023-09-08 02:01:45 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_2_auc_0.9621.pth\n",
      "[DEBUG] 2023-09-08 02:01:45 - Epoch: 3 / 20\n",
      "[DEBUG] 2023-09-08 02:01:45 - --------------------------\n",
      "Epoch 3/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:08:02 - train Loss: 0.1663 AUC: 0.9829\n",
      "Epoch 3/20: 100%|██████████| 435/435 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 02:08:31 - val Loss: 0.2323 AUC: 0.9688\n",
      "[DEBUG] 2023-09-08 02:08:31 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_3_auc_0.9688.pth\n",
      "[DEBUG] 2023-09-08 02:08:31 - Epoch: 4 / 20\n",
      "[DEBUG] 2023-09-08 02:08:31 - --------------------------\n",
      "Epoch 4/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:14:48 - train Loss: 0.0997 AUC: 0.9937\n",
      "Epoch 4/20: 100%|██████████| 435/435 [00:28<00:00, 15.07it/s]\n",
      "[DEBUG] 2023-09-08 02:15:17 - val Loss: 0.1800 AUC: 0.9818\n",
      "[DEBUG] 2023-09-08 02:15:18 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_4_auc_0.9818.pth\n",
      "[DEBUG] 2023-09-08 02:15:18 - Epoch: 5 / 20\n",
      "[DEBUG] 2023-09-08 02:15:18 - --------------------------\n",
      "Epoch 5/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:21:34 - train Loss: 0.0709 AUC: 0.9964\n",
      "Epoch 5/20: 100%|██████████| 435/435 [00:28<00:00, 15.25it/s]\n",
      "[DEBUG] 2023-09-08 02:22:03 - val Loss: 0.1537 AUC: 0.9869\n",
      "[DEBUG] 2023-09-08 02:22:04 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_5_auc_0.9869.pth\n",
      "[DEBUG] 2023-09-08 02:22:04 - Epoch: 6 / 20\n",
      "[DEBUG] 2023-09-08 02:22:04 - --------------------------\n",
      "Epoch 6/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:28:20 - train Loss: 0.0538 AUC: 0.9982\n",
      "Epoch 6/20: 100%|██████████| 435/435 [00:28<00:00, 15.14it/s]\n",
      "[DEBUG] 2023-09-08 02:28:49 - val Loss: 0.1752 AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 02:28:49 - Epoch: 7 / 20\n",
      "[DEBUG] 2023-09-08 02:28:49 - --------------------------\n",
      "Epoch 7/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:35:06 - train Loss: 0.0632 AUC: 0.9969\n",
      "Epoch 7/20: 100%|██████████| 435/435 [00:28<00:00, 15.20it/s]\n",
      "[DEBUG] 2023-09-08 02:35:34 - val Loss: 0.2233 AUC: 0.9741\n",
      "[DEBUG] 2023-09-08 02:35:34 - Epoch: 8 / 20\n",
      "[DEBUG] 2023-09-08 02:35:34 - --------------------------\n",
      "Epoch 8/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:41:51 - train Loss: 0.0288 AUC: 0.9996\n",
      "Epoch 8/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 02:42:19 - val Loss: 0.3397 AUC: 0.9733\n",
      "[DEBUG] 2023-09-08 02:42:19 - Epoch: 9 / 20\n",
      "[DEBUG] 2023-09-08 02:42:19 - --------------------------\n",
      "Epoch 9/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:48:36 - train Loss: 0.0392 AUC: 0.9990\n",
      "Epoch 9/20: 100%|██████████| 435/435 [00:28<00:00, 15.24it/s]\n",
      "[DEBUG] 2023-09-08 02:49:05 - val Loss: 0.2204 AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 02:49:05 - Epoch: 10 / 20\n",
      "[DEBUG] 2023-09-08 02:49:05 - --------------------------\n",
      "Epoch 10/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 02:55:21 - train Loss: 0.0411 AUC: 0.9986\n",
      "Epoch 10/20: 100%|██████████| 435/435 [00:28<00:00, 15.07it/s]\n",
      "[DEBUG] 2023-09-08 02:55:50 - val Loss: 0.1592 AUC: 0.9864\n",
      "[DEBUG] 2023-09-08 02:55:50 - Epoch: 11 / 20\n",
      "[DEBUG] 2023-09-08 02:55:50 - --------------------------\n",
      "Epoch 11/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:02:07 - train Loss: 0.0361 AUC: 0.9986\n",
      "Epoch 11/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 03:02:36 - val Loss: 0.2127 AUC: 0.9807\n",
      "[DEBUG] 2023-09-08 03:02:36 - Epoch: 12 / 20\n",
      "[DEBUG] 2023-09-08 03:02:36 - --------------------------\n",
      "Epoch 12/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:08:53 - train Loss: 0.0321 AUC: 0.9994\n",
      "Epoch 12/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 03:09:22 - val Loss: 0.3216 AUC: 0.9832\n",
      "[DEBUG] 2023-09-08 03:09:22 - Epoch: 13 / 20\n",
      "[DEBUG] 2023-09-08 03:09:22 - --------------------------\n",
      "Epoch 13/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:15:39 - train Loss: 0.0151 AUC: 0.9997\n",
      "Epoch 13/20: 100%|██████████| 435/435 [00:28<00:00, 15.04it/s]\n",
      "[DEBUG] 2023-09-08 03:16:08 - val Loss: 0.2285 AUC: 0.9858\n",
      "[DEBUG] 2023-09-08 03:16:08 - Epoch: 14 / 20\n",
      "[DEBUG] 2023-09-08 03:16:08 - --------------------------\n",
      "Epoch 14/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:22:25 - train Loss: 0.0243 AUC: 0.9997\n",
      "Epoch 14/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 03:22:53 - val Loss: 0.3216 AUC: 0.9736\n",
      "[DEBUG] 2023-09-08 03:22:53 - Epoch: 15 / 20\n",
      "[DEBUG] 2023-09-08 03:22:53 - --------------------------\n",
      "Epoch 15/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:29:11 - train Loss: 0.0282 AUC: 0.9993\n",
      "Epoch 15/20: 100%|██████████| 435/435 [00:28<00:00, 15.21it/s]\n",
      "[DEBUG] 2023-09-08 03:29:39 - val Loss: 0.2317 AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 03:29:39 - Epoch: 16 / 20\n",
      "[DEBUG] 2023-09-08 03:29:39 - --------------------------\n",
      "Epoch 16/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:35:56 - train Loss: 0.0276 AUC: 0.9993\n",
      "Epoch 16/20: 100%|██████████| 435/435 [00:28<00:00, 15.09it/s]\n",
      "[DEBUG] 2023-09-08 03:36:25 - val Loss: 0.1507 AUC: 0.9902\n",
      "[DEBUG] 2023-09-08 03:36:26 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_16_auc_0.9902.pth\n",
      "[DEBUG] 2023-09-08 03:36:26 - Epoch: 17 / 20\n",
      "[DEBUG] 2023-09-08 03:36:26 - --------------------------\n",
      "Epoch 17/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:42:43 - train Loss: 0.0249 AUC: 0.9994\n",
      "Epoch 17/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 03:43:12 - val Loss: 0.2007 AUC: 0.9850\n",
      "[DEBUG] 2023-09-08 03:43:12 - Epoch: 18 / 20\n",
      "[DEBUG] 2023-09-08 03:43:12 - --------------------------\n",
      "Epoch 18/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:49:29 - train Loss: 0.0264 AUC: 0.9995\n",
      "Epoch 18/20: 100%|██████████| 435/435 [00:28<00:00, 15.19it/s]\n",
      "[DEBUG] 2023-09-08 03:49:57 - val Loss: 0.1960 AUC: 0.9885\n",
      "[DEBUG] 2023-09-08 03:49:57 - Epoch: 19 / 20\n",
      "[DEBUG] 2023-09-08 03:49:57 - --------------------------\n",
      "Epoch 19/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 03:56:15 - train Loss: 0.0031 AUC: 1.0000\n",
      "Epoch 19/20: 100%|██████████| 435/435 [00:28<00:00, 15.14it/s]\n",
      "[DEBUG] 2023-09-08 03:56:43 - val Loss: 0.2144 AUC: 0.9908\n",
      "[DEBUG] 2023-09-08 03:56:44 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_19_auc_0.9908.pth\n",
      "[DEBUG] 2023-09-08 03:56:44 - Epoch: 20 / 20\n",
      "[DEBUG] 2023-09-08 03:56:44 - --------------------------\n",
      "Epoch 20/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:03:01 - train Loss: 0.0151 AUC: 0.9999\n",
      "Epoch 20/20: 100%|██████████| 435/435 [00:28<00:00, 15.11it/s]\n",
      "[DEBUG] 2023-09-08 04:03:30 - val Loss: 0.1625 AUC: 0.9897\n",
      "[DEBUG] 2023-09-08 04:03:30 - Training complete. Best AUC: 0.9908\n",
      "[DEBUG] 2023-09-08 04:03:30 - Best model path: c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908014813_epoch_19_auc_0.9908.pth\n",
      "[DEBUG] 2023-09-08 04:03:30 - Fold: 3/5\n",
      "[DEBUG] 2023-09-08 04:03:30 - --------------------\n",
      "[DEBUG] 2023-09-08 04:03:32 - Using device: cuda(NVIDIA GeForce RTX 3080)\n",
      "[DEBUG] 2023-09-08 04:03:32 - Starting training on cuda at 2023-09-08 04:03:32\n",
      "[DEBUG] 2023-09-08 04:03:32 - Model parameters:\n",
      "[DEBUG] 2023-09-08 04:03:32 - patch_embed.proj.weight: torch.Size([192, 3, 4, 4])\n",
      "[DEBUG] 2023-09-08 04:03:32 - patch_embed.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - patch_embed.norm.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - patch_embed.norm.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.0.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.0.blocks.1.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.downsample.reduction.weight: torch.Size([384, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.downsample.norm.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.downsample.norm.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.0.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.1.blocks.1.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.downsample.reduction.weight: torch.Size([768, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.downsample.norm.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.downsample.norm.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.0.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.1.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.2.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.3.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.4.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.5.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.6.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.7.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.8.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.9.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.10.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.11.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.12.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.13.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.14.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.15.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.16.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.2.blocks.17.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.downsample.reduction.weight: torch.Size([1536, 3072])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.downsample.norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.downsample.norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.0.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - layers.3.blocks.1.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - head.fc.weight: torch.Size([2, 1536])\n",
      "[DEBUG] 2023-09-08 04:03:32 - head.fc.bias: torch.Size([2])\n",
      "[DEBUG] 2023-09-08 04:03:32 - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "[DEBUG] 2023-09-08 04:03:32 - Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001B3F2DAFE50>\n",
      "[DEBUG] 2023-09-08 04:03:32 - Loss function: CrossEntropyLoss()\n",
      "[DEBUG] 2023-09-08 04:03:32 - Epoch: 1 / 20\n",
      "[DEBUG] 2023-09-08 04:03:32 - --------------------------\n",
      "Epoch 1/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.61it/s]\n",
      "[DEBUG] 2023-09-08 04:09:50 - train Loss: 0.4774 AUC: 0.8457\n",
      "Epoch 1/20: 100%|██████████| 435/435 [00:28<00:00, 15.22it/s]\n",
      "[DEBUG] 2023-09-08 04:10:18 - val Loss: 0.3772 AUC: 0.9082\n",
      "[DEBUG] 2023-09-08 04:10:19 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_1_auc_0.9082.pth\n",
      "[DEBUG] 2023-09-08 04:10:19 - Epoch: 2 / 20\n",
      "[DEBUG] 2023-09-08 04:10:19 - --------------------------\n",
      "Epoch 2/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:16:36 - train Loss: 0.2789 AUC: 0.9507\n",
      "Epoch 2/20: 100%|██████████| 435/435 [00:28<00:00, 15.12it/s]\n",
      "[DEBUG] 2023-09-08 04:17:05 - val Loss: 0.2930 AUC: 0.9481\n",
      "[DEBUG] 2023-09-08 04:17:06 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_2_auc_0.9481.pth\n",
      "[DEBUG] 2023-09-08 04:17:06 - Epoch: 3 / 20\n",
      "[DEBUG] 2023-09-08 04:17:06 - --------------------------\n",
      "Epoch 3/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:23:22 - train Loss: 0.1418 AUC: 0.9876\n",
      "Epoch 3/20: 100%|██████████| 435/435 [00:28<00:00, 15.24it/s]\n",
      "[DEBUG] 2023-09-08 04:23:51 - val Loss: 0.3250 AUC: 0.9480\n",
      "[DEBUG] 2023-09-08 04:23:51 - Epoch: 4 / 20\n",
      "[DEBUG] 2023-09-08 04:23:51 - --------------------------\n",
      "Epoch 4/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 04:30:07 - train Loss: 0.0925 AUC: 0.9945\n",
      "Epoch 4/20: 100%|██████████| 435/435 [00:28<00:00, 15.04it/s]\n",
      "[DEBUG] 2023-09-08 04:30:36 - val Loss: 0.3761 AUC: 0.9508\n",
      "[DEBUG] 2023-09-08 04:30:37 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_4_auc_0.9508.pth\n",
      "[DEBUG] 2023-09-08 04:30:37 - Epoch: 5 / 20\n",
      "[DEBUG] 2023-09-08 04:30:37 - --------------------------\n",
      "Epoch 5/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:36:54 - train Loss: 0.0734 AUC: 0.9960\n",
      "Epoch 5/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 04:37:23 - val Loss: 0.2674 AUC: 0.9651\n",
      "[DEBUG] 2023-09-08 04:37:23 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_5_auc_0.9651.pth\n",
      "[DEBUG] 2023-09-08 04:37:23 - Epoch: 6 / 20\n",
      "[DEBUG] 2023-09-08 04:37:23 - --------------------------\n",
      "Epoch 6/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:43:40 - train Loss: 0.0658 AUC: 0.9970\n",
      "Epoch 6/20: 100%|██████████| 435/435 [00:28<00:00, 15.21it/s]\n",
      "[DEBUG] 2023-09-08 04:44:09 - val Loss: 0.2393 AUC: 0.9696\n",
      "[DEBUG] 2023-09-08 04:44:09 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_6_auc_0.9696.pth\n",
      "[DEBUG] 2023-09-08 04:44:09 - Epoch: 7 / 20\n",
      "[DEBUG] 2023-09-08 04:44:09 - --------------------------\n",
      "Epoch 7/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:50:26 - train Loss: 0.0427 AUC: 0.9979\n",
      "Epoch 7/20: 100%|██████████| 435/435 [00:28<00:00, 15.11it/s]\n",
      "[DEBUG] 2023-09-08 04:50:55 - val Loss: 0.2889 AUC: 0.9634\n",
      "[DEBUG] 2023-09-08 04:50:55 - Epoch: 8 / 20\n",
      "[DEBUG] 2023-09-08 04:50:55 - --------------------------\n",
      "Epoch 8/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 04:57:12 - train Loss: 0.0213 AUC: 0.9998\n",
      "Epoch 8/20: 100%|██████████| 435/435 [00:28<00:00, 15.12it/s]\n",
      "[DEBUG] 2023-09-08 04:57:41 - val Loss: 0.4757 AUC: 0.9629\n",
      "[DEBUG] 2023-09-08 04:57:41 - Epoch: 9 / 20\n",
      "[DEBUG] 2023-09-08 04:57:41 - --------------------------\n",
      "Epoch 9/20: 100%|██████████| 1741/1741 [06:16<00:00,  4.62it/s]\n",
      "[DEBUG] 2023-09-08 05:03:57 - train Loss: 0.0820 AUC: 0.9959\n",
      "Epoch 9/20: 100%|██████████| 435/435 [00:28<00:00, 15.11it/s]\n",
      "[DEBUG] 2023-09-08 05:04:26 - val Loss: 0.2519 AUC: 0.9714\n",
      "[DEBUG] 2023-09-08 05:04:27 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_9_auc_0.9714.pth\n",
      "[DEBUG] 2023-09-08 05:04:27 - Epoch: 10 / 20\n",
      "[DEBUG] 2023-09-08 05:04:27 - --------------------------\n",
      "Epoch 10/20: 100%|██████████| 1741/1741 [06:17<00:00,  4.61it/s]\n",
      "[DEBUG] 2023-09-08 05:10:45 - train Loss: 0.0257 AUC: 0.9996\n",
      "Epoch 10/20: 100%|██████████| 435/435 [00:28<00:00, 15.12it/s]\n",
      "[DEBUG] 2023-09-08 05:11:13 - val Loss: 0.3500 AUC: 0.9627\n",
      "[DEBUG] 2023-09-08 05:11:13 - Epoch: 11 / 20\n",
      "[DEBUG] 2023-09-08 05:11:13 - --------------------------\n",
      "Epoch 11/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 05:17:32 - train Loss: 0.0323 AUC: 0.9992\n",
      "Epoch 11/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 05:18:01 - val Loss: 0.3090 AUC: 0.9671\n",
      "[DEBUG] 2023-09-08 05:18:01 - Epoch: 12 / 20\n",
      "[DEBUG] 2023-09-08 05:18:01 - --------------------------\n",
      "Epoch 12/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 05:24:20 - train Loss: 0.0265 AUC: 0.9994\n",
      "Epoch 12/20: 100%|██████████| 435/435 [00:28<00:00, 15.21it/s]\n",
      "[DEBUG] 2023-09-08 05:24:49 - val Loss: 0.2857 AUC: 0.9719\n",
      "[DEBUG] 2023-09-08 05:24:49 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_12_auc_0.9719.pth\n",
      "[DEBUG] 2023-09-08 05:24:49 - Epoch: 13 / 20\n",
      "[DEBUG] 2023-09-08 05:24:49 - --------------------------\n",
      "Epoch 13/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 05:31:08 - train Loss: 0.0309 AUC: 0.9992\n",
      "Epoch 13/20: 100%|██████████| 435/435 [00:28<00:00, 15.14it/s]\n",
      "[DEBUG] 2023-09-08 05:31:37 - val Loss: 0.2873 AUC: 0.9717\n",
      "[DEBUG] 2023-09-08 05:31:37 - Epoch: 14 / 20\n",
      "[DEBUG] 2023-09-08 05:31:37 - --------------------------\n",
      "Epoch 14/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 05:37:56 - train Loss: 0.0388 AUC: 0.9991\n",
      "Epoch 14/20: 100%|██████████| 435/435 [00:28<00:00, 15.09it/s]\n",
      "[DEBUG] 2023-09-08 05:38:25 - val Loss: 0.3507 AUC: 0.9638\n",
      "[DEBUG] 2023-09-08 05:38:25 - Epoch: 15 / 20\n",
      "[DEBUG] 2023-09-08 05:38:25 - --------------------------\n",
      "Epoch 15/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 05:44:44 - train Loss: 0.0150 AUC: 0.9998\n",
      "Epoch 15/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 05:45:13 - val Loss: 0.3323 AUC: 0.9745\n",
      "[DEBUG] 2023-09-08 05:45:13 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_15_auc_0.9745.pth\n",
      "[DEBUG] 2023-09-08 05:45:13 - Epoch: 16 / 20\n",
      "[DEBUG] 2023-09-08 05:45:13 - --------------------------\n",
      "Epoch 16/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 05:51:32 - train Loss: 0.0205 AUC: 0.9997\n",
      "Epoch 16/20: 100%|██████████| 435/435 [00:28<00:00, 15.09it/s]\n",
      "[DEBUG] 2023-09-08 05:52:01 - val Loss: 0.3336 AUC: 0.9721\n",
      "[DEBUG] 2023-09-08 05:52:01 - Epoch: 17 / 20\n",
      "[DEBUG] 2023-09-08 05:52:01 - --------------------------\n",
      "Epoch 17/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 05:58:20 - train Loss: 0.0493 AUC: 0.9981\n",
      "Epoch 17/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 05:58:49 - val Loss: 0.2276 AUC: 0.9809\n",
      "[DEBUG] 2023-09-08 05:58:50 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_17_auc_0.9809.pth\n",
      "[DEBUG] 2023-09-08 05:58:50 - Epoch: 18 / 20\n",
      "[DEBUG] 2023-09-08 05:58:50 - --------------------------\n",
      "Epoch 18/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 06:05:09 - train Loss: 0.0229 AUC: 0.9996\n",
      "Epoch 18/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 06:05:37 - val Loss: 0.2773 AUC: 0.9747\n",
      "[DEBUG] 2023-09-08 06:05:37 - Epoch: 19 / 20\n",
      "[DEBUG] 2023-09-08 06:05:37 - --------------------------\n",
      "Epoch 19/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 06:11:57 - train Loss: 0.0135 AUC: 0.9999\n",
      "Epoch 19/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 06:12:25 - val Loss: 0.2621 AUC: 0.9801\n",
      "[DEBUG] 2023-09-08 06:12:25 - Epoch: 20 / 20\n",
      "[DEBUG] 2023-09-08 06:12:25 - --------------------------\n",
      "Epoch 20/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.58it/s]\n",
      "[DEBUG] 2023-09-08 06:18:45 - train Loss: 0.0044 AUC: 1.0000\n",
      "Epoch 20/20: 100%|██████████| 435/435 [00:28<00:00, 15.09it/s]\n",
      "[DEBUG] 2023-09-08 06:19:14 - val Loss: 0.2846 AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 06:19:15 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_20_auc_0.9838.pth\n",
      "[DEBUG] 2023-09-08 06:19:15 - Training complete. Best AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 06:19:15 - Best model path: c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908040332_epoch_20_auc_0.9838.pth\n",
      "[DEBUG] 2023-09-08 06:19:15 - Fold: 4/5\n",
      "[DEBUG] 2023-09-08 06:19:15 - --------------------\n",
      "[DEBUG] 2023-09-08 06:19:16 - Using device: cuda(NVIDIA GeForce RTX 3080)\n",
      "[DEBUG] 2023-09-08 06:19:16 - Starting training on cuda at 2023-09-08 06:19:16\n",
      "[DEBUG] 2023-09-08 06:19:16 - Model parameters:\n",
      "[DEBUG] 2023-09-08 06:19:16 - patch_embed.proj.weight: torch.Size([192, 3, 4, 4])\n",
      "[DEBUG] 2023-09-08 06:19:16 - patch_embed.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - patch_embed.norm.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - patch_embed.norm.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.0.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.0.blocks.1.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.downsample.reduction.weight: torch.Size([384, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.downsample.norm.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.downsample.norm.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.0.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.1.blocks.1.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.downsample.reduction.weight: torch.Size([768, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.downsample.norm.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.downsample.norm.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.0.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.1.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.2.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.3.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.4.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.5.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.6.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.7.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.8.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.9.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.10.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.11.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.12.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.13.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:16 - layers.2.blocks.14.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.14.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.14.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.14.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.14.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.14.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.14.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.15.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.16.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.2.blocks.17.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.downsample.reduction.weight: torch.Size([1536, 3072])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.downsample.norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.downsample.norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.0.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - layers.3.blocks.1.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - head.fc.weight: torch.Size([2, 1536])\n",
      "[DEBUG] 2023-09-08 06:19:17 - head.fc.bias: torch.Size([2])\n",
      "[DEBUG] 2023-09-08 06:19:17 - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "[DEBUG] 2023-09-08 06:19:17 - Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001B3EF1C0810>\n",
      "[DEBUG] 2023-09-08 06:19:17 - Loss function: CrossEntropyLoss()\n",
      "[DEBUG] 2023-09-08 06:19:17 - Epoch: 1 / 20\n",
      "[DEBUG] 2023-09-08 06:19:17 - --------------------------\n",
      "Epoch 1/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 06:25:36 - train Loss: 0.4968 AUC: 0.8340\n",
      "Epoch 1/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 06:26:05 - val Loss: 0.4194 AUC: 0.9152\n",
      "[DEBUG] 2023-09-08 06:26:05 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_1_auc_0.9152.pth\n",
      "[DEBUG] 2023-09-08 06:26:05 - Epoch: 2 / 20\n",
      "[DEBUG] 2023-09-08 06:26:05 - --------------------------\n",
      "Epoch 2/20: 100%|██████████| 1741/1741 [06:20<00:00,  4.58it/s]\n",
      "[DEBUG] 2023-09-08 06:32:26 - train Loss: 0.3204 AUC: 0.9335\n",
      "Epoch 2/20: 100%|██████████| 435/435 [00:28<00:00, 15.07it/s]\n",
      "[DEBUG] 2023-09-08 06:32:55 - val Loss: 0.2837 AUC: 0.9517\n",
      "[DEBUG] 2023-09-08 06:32:55 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_2_auc_0.9517.pth\n",
      "[DEBUG] 2023-09-08 06:32:55 - Epoch: 3 / 20\n",
      "[DEBUG] 2023-09-08 06:32:55 - --------------------------\n",
      "Epoch 3/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 06:39:14 - train Loss: 0.1713 AUC: 0.9809\n",
      "Epoch 3/20: 100%|██████████| 435/435 [00:28<00:00, 15.19it/s]\n",
      "[DEBUG] 2023-09-08 06:39:43 - val Loss: 0.2643 AUC: 0.9674\n",
      "[DEBUG] 2023-09-08 06:39:43 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_3_auc_0.9674.pth\n",
      "[DEBUG] 2023-09-08 06:39:43 - Epoch: 4 / 20\n",
      "[DEBUG] 2023-09-08 06:39:43 - --------------------------\n",
      "Epoch 4/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 06:46:02 - train Loss: 0.0977 AUC: 0.9935\n",
      "Epoch 4/20: 100%|██████████| 435/435 [00:28<00:00, 15.10it/s]\n",
      "[DEBUG] 2023-09-08 06:46:31 - val Loss: 0.2700 AUC: 0.9624\n",
      "[DEBUG] 2023-09-08 06:46:31 - Epoch: 5 / 20\n",
      "[DEBUG] 2023-09-08 06:46:31 - --------------------------\n",
      "Epoch 5/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 06:52:50 - train Loss: 0.0824 AUC: 0.9959\n",
      "Epoch 5/20: 100%|██████████| 435/435 [00:28<00:00, 15.12it/s]\n",
      "[DEBUG] 2023-09-08 06:53:19 - val Loss: 0.2398 AUC: 0.9718\n",
      "[DEBUG] 2023-09-08 06:53:20 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_5_auc_0.9718.pth\n",
      "[DEBUG] 2023-09-08 06:53:20 - Epoch: 6 / 20\n",
      "[DEBUG] 2023-09-08 06:53:20 - --------------------------\n",
      "Epoch 6/20: 100%|██████████| 1741/1741 [06:19<00:00,  4.59it/s]\n",
      "[DEBUG] 2023-09-08 06:59:39 - train Loss: 0.0720 AUC: 0.9968\n",
      "Epoch 6/20: 100%|██████████| 435/435 [00:28<00:00, 15.37it/s]\n",
      "[DEBUG] 2023-09-08 07:00:07 - val Loss: 0.2818 AUC: 0.9709\n",
      "[DEBUG] 2023-09-08 07:00:07 - Epoch: 7 / 20\n",
      "[DEBUG] 2023-09-08 07:00:07 - --------------------------\n",
      "Epoch 7/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 07:06:25 - train Loss: 0.0512 AUC: 0.9979\n",
      "Epoch 7/20: 100%|██████████| 435/435 [00:28<00:00, 15.11it/s]\n",
      "[DEBUG] 2023-09-08 07:06:54 - val Loss: 0.3363 AUC: 0.9583\n",
      "[DEBUG] 2023-09-08 07:06:54 - Epoch: 8 / 20\n",
      "[DEBUG] 2023-09-08 07:06:54 - --------------------------\n",
      "Epoch 8/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 07:13:13 - train Loss: 0.0472 AUC: 0.9976\n",
      "Epoch 8/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 07:13:41 - val Loss: 0.2380 AUC: 0.9757\n",
      "[DEBUG] 2023-09-08 07:13:42 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_8_auc_0.9757.pth\n",
      "[DEBUG] 2023-09-08 07:13:42 - Epoch: 9 / 20\n",
      "[DEBUG] 2023-09-08 07:13:42 - --------------------------\n",
      "Epoch 9/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 07:20:00 - train Loss: 0.0219 AUC: 0.9997\n",
      "Epoch 9/20: 100%|██████████| 435/435 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 07:20:29 - val Loss: 0.5175 AUC: 0.9688\n",
      "[DEBUG] 2023-09-08 07:20:29 - Epoch: 10 / 20\n",
      "[DEBUG] 2023-09-08 07:20:29 - --------------------------\n",
      "Epoch 10/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 07:26:47 - train Loss: 0.0490 AUC: 0.9984\n",
      "Epoch 10/20: 100%|██████████| 435/435 [00:28<00:00, 15.20it/s]\n",
      "[DEBUG] 2023-09-08 07:27:16 - val Loss: 0.1988 AUC: 0.9797\n",
      "[DEBUG] 2023-09-08 07:27:17 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_10_auc_0.9797.pth\n",
      "[DEBUG] 2023-09-08 07:27:17 - Epoch: 11 / 20\n",
      "[DEBUG] 2023-09-08 07:27:17 - --------------------------\n",
      "Epoch 11/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.61it/s]\n",
      "[DEBUG] 2023-09-08 07:33:35 - train Loss: 0.0187 AUC: 0.9998\n",
      "Epoch 11/20: 100%|██████████| 435/435 [00:28<00:00, 15.24it/s]\n",
      "[DEBUG] 2023-09-08 07:34:03 - val Loss: 0.3339 AUC: 0.9730\n",
      "[DEBUG] 2023-09-08 07:34:03 - Epoch: 12 / 20\n",
      "[DEBUG] 2023-09-08 07:34:03 - --------------------------\n",
      "Epoch 12/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 07:40:22 - train Loss: 0.0415 AUC: 0.9989\n",
      "Epoch 12/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 07:40:50 - val Loss: 0.1973 AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 07:40:51 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_12_auc_0.9838.pth\n",
      "[DEBUG] 2023-09-08 07:40:51 - Epoch: 13 / 20\n",
      "[DEBUG] 2023-09-08 07:40:51 - --------------------------\n",
      "Epoch 13/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 07:47:09 - train Loss: 0.0280 AUC: 0.9992\n",
      "Epoch 13/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 07:47:38 - val Loss: 0.2101 AUC: 0.9806\n",
      "[DEBUG] 2023-09-08 07:47:38 - Epoch: 14 / 20\n",
      "[DEBUG] 2023-09-08 07:47:38 - --------------------------\n",
      "Epoch 14/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.61it/s]\n",
      "[DEBUG] 2023-09-08 07:53:56 - train Loss: 0.0241 AUC: 0.9990\n",
      "Epoch 14/20: 100%|██████████| 435/435 [00:28<00:00, 15.21it/s]\n",
      "[DEBUG] 2023-09-08 07:54:25 - val Loss: 0.2258 AUC: 0.9800\n",
      "[DEBUG] 2023-09-08 07:54:25 - Epoch: 15 / 20\n",
      "[DEBUG] 2023-09-08 07:54:25 - --------------------------\n",
      "Epoch 15/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 08:00:43 - train Loss: 0.0276 AUC: 0.9995\n",
      "Epoch 15/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 08:01:12 - val Loss: 0.2540 AUC: 0.9812\n",
      "[DEBUG] 2023-09-08 08:01:12 - Epoch: 16 / 20\n",
      "[DEBUG] 2023-09-08 08:01:12 - --------------------------\n",
      "Epoch 16/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 08:07:30 - train Loss: 0.0317 AUC: 0.9992\n",
      "Epoch 16/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 08:07:59 - val Loss: 0.2644 AUC: 0.9755\n",
      "[DEBUG] 2023-09-08 08:07:59 - Epoch: 17 / 20\n",
      "[DEBUG] 2023-09-08 08:07:59 - --------------------------\n",
      "Epoch 17/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 08:14:17 - train Loss: 0.0348 AUC: 0.9986\n",
      "Epoch 17/20: 100%|██████████| 435/435 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 08:14:46 - val Loss: 0.2270 AUC: 0.9832\n",
      "[DEBUG] 2023-09-08 08:14:46 - Epoch: 18 / 20\n",
      "[DEBUG] 2023-09-08 08:14:46 - --------------------------\n",
      "Epoch 18/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 08:21:04 - train Loss: 0.0116 AUC: 0.9999\n",
      "Epoch 18/20: 100%|██████████| 435/435 [00:28<00:00, 15.22it/s]\n",
      "[DEBUG] 2023-09-08 08:21:32 - val Loss: 0.3248 AUC: 0.9766\n",
      "[DEBUG] 2023-09-08 08:21:32 - Epoch: 19 / 20\n",
      "[DEBUG] 2023-09-08 08:21:32 - --------------------------\n",
      "Epoch 19/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 08:27:51 - train Loss: 0.0385 AUC: 0.9983\n",
      "Epoch 19/20: 100%|██████████| 435/435 [00:28<00:00, 15.22it/s]\n",
      "[DEBUG] 2023-09-08 08:28:19 - val Loss: 0.2901 AUC: 0.9778\n",
      "[DEBUG] 2023-09-08 08:28:19 - Epoch: 20 / 20\n",
      "[DEBUG] 2023-09-08 08:28:19 - --------------------------\n",
      "Epoch 20/20: 100%|██████████| 1741/1741 [06:18<00:00,  4.60it/s]\n",
      "[DEBUG] 2023-09-08 08:34:38 - train Loss: 0.0026 AUC: 1.0000\n",
      "Epoch 20/20: 100%|██████████| 435/435 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 08:35:06 - val Loss: 0.3594 AUC: 0.9802\n",
      "[DEBUG] 2023-09-08 08:35:06 - Training complete. Best AUC: 0.9838\n",
      "[DEBUG] 2023-09-08 08:35:06 - Best model path: c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908061916_epoch_12_auc_0.9838.pth\n",
      "[DEBUG] 2023-09-08 08:35:06 - Fold: 5/5\n",
      "[DEBUG] 2023-09-08 08:35:06 - --------------------\n",
      "[DEBUG] 2023-09-08 08:35:08 - Using device: cuda(NVIDIA GeForce RTX 3080)\n",
      "[DEBUG] 2023-09-08 08:35:08 - Starting training on cuda at 2023-09-08 08:35:08\n",
      "[DEBUG] 2023-09-08 08:35:08 - Model parameters:\n",
      "[DEBUG] 2023-09-08 08:35:08 - patch_embed.proj.weight: torch.Size([192, 3, 4, 4])\n",
      "[DEBUG] 2023-09-08 08:35:08 - patch_embed.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - patch_embed.norm.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - patch_embed.norm.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.0.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.logit_scale: torch.Size([6, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.q_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.v_bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.cpb_mlp.2.weight: torch.Size([6, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.qkv.weight: torch.Size([576, 192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.proj.weight: torch.Size([192, 192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.attn.proj.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.norm1.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.norm1.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.mlp.fc1.weight: torch.Size([768, 192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.mlp.fc1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.mlp.fc2.weight: torch.Size([192, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.mlp.fc2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.norm2.weight: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.0.blocks.1.norm2.bias: torch.Size([192])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.downsample.reduction.weight: torch.Size([384, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.downsample.norm.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.downsample.norm.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.0.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.logit_scale: torch.Size([12, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.q_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.v_bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.cpb_mlp.2.weight: torch.Size([12, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.qkv.weight: torch.Size([1152, 384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.proj.weight: torch.Size([384, 384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.attn.proj.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.norm1.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.norm1.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.mlp.fc1.weight: torch.Size([1536, 384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.mlp.fc1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.mlp.fc2.weight: torch.Size([384, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.mlp.fc2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.norm2.weight: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.1.blocks.1.norm2.bias: torch.Size([384])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.downsample.reduction.weight: torch.Size([768, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.downsample.norm.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.downsample.norm.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.0.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.1.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.2.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.3.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.4.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.5.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.6.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.7.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.8.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.9.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.10.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.11.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.12.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.13.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.14.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.15.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.16.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.logit_scale: torch.Size([24, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.q_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.v_bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.cpb_mlp.2.weight: torch.Size([24, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.qkv.weight: torch.Size([2304, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.proj.weight: torch.Size([768, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.attn.proj.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.norm1.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.norm1.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.mlp.fc1.bias: torch.Size([3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.mlp.fc2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.norm2.weight: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.2.blocks.17.norm2.bias: torch.Size([768])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.downsample.reduction.weight: torch.Size([1536, 3072])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.downsample.norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.downsample.norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.0.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.logit_scale: torch.Size([48, 1, 1])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.q_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.v_bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.cpb_mlp.0.weight: torch.Size([512, 2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.cpb_mlp.0.bias: torch.Size([512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.cpb_mlp.2.weight: torch.Size([48, 512])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.qkv.weight: torch.Size([4608, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.proj.weight: torch.Size([1536, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.attn.proj.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.norm1.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.norm1.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.mlp.fc1.weight: torch.Size([6144, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.mlp.fc1.bias: torch.Size([6144])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.mlp.fc2.weight: torch.Size([1536, 6144])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.mlp.fc2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.norm2.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - layers.3.blocks.1.norm2.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - norm.weight: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - norm.bias: torch.Size([1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - head.fc.weight: torch.Size([2, 1536])\n",
      "[DEBUG] 2023-09-08 08:35:08 - head.fc.bias: torch.Size([2])\n",
      "[DEBUG] 2023-09-08 08:35:08 - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "[DEBUG] 2023-09-08 08:35:08 - Scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001B3F1EE0090>\n",
      "[DEBUG] 2023-09-08 08:35:08 - Loss function: CrossEntropyLoss()\n",
      "[DEBUG] 2023-09-08 08:35:08 - Epoch: 1 / 20\n",
      "[DEBUG] 2023-09-08 08:35:08 - --------------------------\n",
      "Epoch 1/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 08:41:24 - train Loss: 0.4802 AUC: 0.8459\n",
      "Epoch 1/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 08:41:53 - val Loss: 0.3866 AUC: 0.9078\n",
      "[DEBUG] 2023-09-08 08:41:53 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_1_auc_0.9078.pth\n",
      "[DEBUG] 2023-09-08 08:41:53 - Epoch: 2 / 20\n",
      "[DEBUG] 2023-09-08 08:41:53 - --------------------------\n",
      "Epoch 2/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 08:48:09 - train Loss: 0.2858 AUC: 0.9478\n",
      "Epoch 2/20: 100%|██████████| 435/435 [00:28<00:00, 15.18it/s]\n",
      "[DEBUG] 2023-09-08 08:48:37 - val Loss: 0.2429 AUC: 0.9615\n",
      "[DEBUG] 2023-09-08 08:48:38 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_2_auc_0.9615.pth\n",
      "[DEBUG] 2023-09-08 08:48:38 - Epoch: 3 / 20\n",
      "[DEBUG] 2023-09-08 08:48:38 - --------------------------\n",
      "Epoch 3/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 08:54:54 - train Loss: 0.1492 AUC: 0.9857\n",
      "Epoch 3/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 08:55:22 - val Loss: 0.2334 AUC: 0.9686\n",
      "[DEBUG] 2023-09-08 08:55:23 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_3_auc_0.9686.pth\n",
      "[DEBUG] 2023-09-08 08:55:23 - Epoch: 4 / 20\n",
      "[DEBUG] 2023-09-08 08:55:23 - --------------------------\n",
      "Epoch 4/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:01:39 - train Loss: 0.0904 AUC: 0.9944\n",
      "Epoch 4/20: 100%|██████████| 435/435 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 09:02:08 - val Loss: 0.2198 AUC: 0.9736\n",
      "[DEBUG] 2023-09-08 09:02:08 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_4_auc_0.9736.pth\n",
      "[DEBUG] 2023-09-08 09:02:08 - Epoch: 5 / 20\n",
      "[DEBUG] 2023-09-08 09:02:08 - --------------------------\n",
      "Epoch 5/20: 100%|██████████| 1741/1741 [06:14<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 09:08:23 - train Loss: 0.0761 AUC: 0.9958\n",
      "Epoch 5/20: 100%|██████████| 435/435 [00:28<00:00, 15.10it/s]\n",
      "[DEBUG] 2023-09-08 09:08:52 - val Loss: 0.2765 AUC: 0.9748\n",
      "[DEBUG] 2023-09-08 09:08:53 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_5_auc_0.9748.pth\n",
      "[DEBUG] 2023-09-08 09:08:53 - Epoch: 6 / 20\n",
      "[DEBUG] 2023-09-08 09:08:53 - --------------------------\n",
      "Epoch 6/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:15:08 - train Loss: 0.0629 AUC: 0.9965\n",
      "Epoch 6/20: 100%|██████████| 435/435 [00:28<00:00, 15.20it/s]\n",
      "[DEBUG] 2023-09-08 09:15:37 - val Loss: 0.2504 AUC: 0.9666\n",
      "[DEBUG] 2023-09-08 09:15:37 - Epoch: 7 / 20\n",
      "[DEBUG] 2023-09-08 09:15:37 - --------------------------\n",
      "Epoch 7/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:21:53 - train Loss: 0.0576 AUC: 0.9974\n",
      "Epoch 7/20: 100%|██████████| 435/435 [00:28<00:00, 15.22it/s]\n",
      "[DEBUG] 2023-09-08 09:22:21 - val Loss: 0.5908 AUC: 0.9584\n",
      "[DEBUG] 2023-09-08 09:22:21 - Epoch: 8 / 20\n",
      "[DEBUG] 2023-09-08 09:22:21 - --------------------------\n",
      "Epoch 8/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:28:37 - train Loss: 0.0340 AUC: 0.9993\n",
      "Epoch 8/20: 100%|██████████| 435/435 [00:28<00:00, 15.13it/s]\n",
      "[DEBUG] 2023-09-08 09:29:06 - val Loss: 0.3943 AUC: 0.9627\n",
      "[DEBUG] 2023-09-08 09:29:06 - Epoch: 9 / 20\n",
      "[DEBUG] 2023-09-08 09:29:06 - --------------------------\n",
      "Epoch 9/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:35:22 - train Loss: 0.0504 AUC: 0.9981\n",
      "Epoch 9/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 09:35:50 - val Loss: 0.2501 AUC: 0.9775\n",
      "[DEBUG] 2023-09-08 09:35:51 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_9_auc_0.9775.pth\n",
      "[DEBUG] 2023-09-08 09:35:51 - Epoch: 10 / 20\n",
      "[DEBUG] 2023-09-08 09:35:51 - --------------------------\n",
      "Epoch 10/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:42:07 - train Loss: 0.0451 AUC: 0.9982\n",
      "Epoch 10/20: 100%|██████████| 435/435 [00:28<00:00, 15.13it/s]\n",
      "[DEBUG] 2023-09-08 09:42:36 - val Loss: 0.1841 AUC: 0.9847\n",
      "[DEBUG] 2023-09-08 09:42:36 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_10_auc_0.9847.pth\n",
      "[DEBUG] 2023-09-08 09:42:36 - Epoch: 11 / 20\n",
      "[DEBUG] 2023-09-08 09:42:36 - --------------------------\n",
      "Epoch 11/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:48:52 - train Loss: 0.0290 AUC: 0.9995\n",
      "Epoch 11/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 09:49:21 - val Loss: 0.7496 AUC: 0.9582\n",
      "[DEBUG] 2023-09-08 09:49:21 - Epoch: 12 / 20\n",
      "[DEBUG] 2023-09-08 09:49:21 - --------------------------\n",
      "Epoch 12/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 09:55:37 - train Loss: 0.0285 AUC: 0.9994\n",
      "Epoch 12/20: 100%|██████████| 435/435 [00:28<00:00, 15.17it/s]\n",
      "[DEBUG] 2023-09-08 09:56:06 - val Loss: 0.2184 AUC: 0.9794\n",
      "[DEBUG] 2023-09-08 09:56:06 - Epoch: 13 / 20\n",
      "[DEBUG] 2023-09-08 09:56:06 - --------------------------\n",
      "Epoch 13/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 10:02:21 - train Loss: 0.0188 AUC: 0.9997\n",
      "Epoch 13/20: 100%|██████████| 435/435 [00:28<00:00, 15.09it/s]\n",
      "[DEBUG] 2023-09-08 10:02:50 - val Loss: 0.2905 AUC: 0.9761\n",
      "[DEBUG] 2023-09-08 10:02:50 - Epoch: 14 / 20\n",
      "[DEBUG] 2023-09-08 10:02:50 - --------------------------\n",
      "Epoch 14/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 10:09:06 - train Loss: 0.0090 AUC: 0.9999\n",
      "Epoch 14/20: 100%|██████████| 435/435 [00:28<00:00, 15.12it/s]\n",
      "[DEBUG] 2023-09-08 10:09:35 - val Loss: 0.5536 AUC: 0.9761\n",
      "[DEBUG] 2023-09-08 10:09:35 - Epoch: 15 / 20\n",
      "[DEBUG] 2023-09-08 10:09:35 - --------------------------\n",
      "Epoch 15/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 10:15:51 - train Loss: 0.0023 AUC: 1.0000\n",
      "Epoch 15/20: 100%|██████████| 435/435 [00:28<00:00, 15.19it/s]\n",
      "[DEBUG] 2023-09-08 10:16:20 - val Loss: 0.2118 AUC: 0.9872\n",
      "[DEBUG] 2023-09-08 10:16:20 - New best model saved at c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_15_auc_0.9872.pth\n",
      "[DEBUG] 2023-09-08 10:16:20 - Epoch: 16 / 20\n",
      "[DEBUG] 2023-09-08 10:16:20 - --------------------------\n",
      "Epoch 16/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 10:22:36 - train Loss: 0.0237 AUC: 0.9991\n",
      "Epoch 16/20: 100%|██████████| 435/435 [00:28<00:00, 15.19it/s]\n",
      "[DEBUG] 2023-09-08 10:23:05 - val Loss: 0.3222 AUC: 0.9771\n",
      "[DEBUG] 2023-09-08 10:23:05 - Epoch: 17 / 20\n",
      "[DEBUG] 2023-09-08 10:23:05 - --------------------------\n",
      "Epoch 17/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 10:29:21 - train Loss: 0.0251 AUC: 0.9993\n",
      "Epoch 17/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 10:29:49 - val Loss: 0.2974 AUC: 0.9816\n",
      "[DEBUG] 2023-09-08 10:29:49 - Epoch: 18 / 20\n",
      "[DEBUG] 2023-09-08 10:29:49 - --------------------------\n",
      "Epoch 18/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 10:36:05 - train Loss: 0.0264 AUC: 0.9992\n",
      "Epoch 18/20: 100%|██████████| 435/435 [00:28<00:00, 15.15it/s]\n",
      "[DEBUG] 2023-09-08 10:36:34 - val Loss: 0.2704 AUC: 0.9678\n",
      "[DEBUG] 2023-09-08 10:36:34 - Epoch: 19 / 20\n",
      "[DEBUG] 2023-09-08 10:36:34 - --------------------------\n",
      "Epoch 19/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.64it/s]\n",
      "[DEBUG] 2023-09-08 10:42:49 - train Loss: 0.0387 AUC: 0.9985\n",
      "Epoch 19/20: 100%|██████████| 435/435 [00:28<00:00, 15.16it/s]\n",
      "[DEBUG] 2023-09-08 10:43:18 - val Loss: 0.2584 AUC: 0.9734\n",
      "[DEBUG] 2023-09-08 10:43:18 - Epoch: 20 / 20\n",
      "[DEBUG] 2023-09-08 10:43:18 - --------------------------\n",
      "Epoch 20/20: 100%|██████████| 1741/1741 [06:15<00:00,  4.63it/s]\n",
      "[DEBUG] 2023-09-08 10:49:34 - train Loss: 0.0096 AUC: 0.9999\n",
      "Epoch 20/20: 100%|██████████| 435/435 [00:28<00:00, 15.14it/s]\n",
      "[DEBUG] 2023-09-08 10:50:02 - val Loss: 0.2999 AUC: 0.9758\n",
      "[DEBUG] 2023-09-08 10:50:02 - Training complete. Best AUC: 0.9872\n",
      "[DEBUG] 2023-09-08 10:50:02 - Best model path: c:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\../../data/models/SwinV2Model_cv\\swintransformerv2_20230908083508_epoch_15_auc_0.9872.pth\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\SwinV2ModelCV.ipynb セル 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/negi/Documents/GitHub/tkser/technopro-food-package/notebook/SwinV2ModelCV.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m package_model_path, loss_history, auc_history \u001b[39m=\u001b[39m train_SwinV2Model(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/negi/Documents/GitHub/tkser/technopro-food-package/notebook/SwinV2ModelCV.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/negi/Documents/GitHub/tkser/technopro-food-package/notebook/SwinV2ModelCV.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-05\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/negi/Documents/GitHub/tkser/technopro-food-package/notebook/SwinV2ModelCV.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/negi/Documents/GitHub/tkser/technopro-food-package/notebook/SwinV2ModelCV.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_splits\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/negi/Documents/GitHub/tkser/technopro-food-package/notebook/SwinV2ModelCV.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\notebook\\../src\\models\\SwinV2Model\\train_cv.py:97\u001b[0m, in \u001b[0;36mtrain_cv\u001b[1;34m(batch_size, learning_rate, num_epochs, seed, lr_min, model_name, n_splits)\u001b[0m\n\u001b[0;32m     93\u001b[0m     auc_histories\u001b[39m.\u001b[39mappend(auc_history)\n\u001b[0;32m     95\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest model path: \u001b[39m\u001b[39m{\u001b[39;00mbest_model_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m loss_history_avg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(loss_histories, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     98\u001b[0m auc_history_avg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(auc_histories, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    100\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCV AUC: \u001b[39m\u001b[39m{\u001b[39;00mroc_auc_score(cv_true,\u001b[39m \u001b[39mcv_pred)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\negi\\Documents\\GitHub\\tkser\\technopro-food-package\\.venv\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "package_model_path, loss_history, auc_history = train_SwinV2Model(\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-05,\n",
    "    batch_size=1,\n",
    "    n_splits=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2023-09-08 12:01:19 - Starting prediction on cuda\n",
      "Prediction: 100%|██████████| 2180/2180 [02:30<00:00, 14.49it/s]\n",
      "[DEBUG] 2023-09-08 12:03:50 - Finished prediction\n",
      "[DEBUG] 2023-09-08 12:03:52 - Starting prediction on cuda\n",
      "Prediction: 100%|██████████| 2180/2180 [02:28<00:00, 14.71it/s]\n",
      "[DEBUG] 2023-09-08 12:06:21 - Finished prediction\n",
      "[DEBUG] 2023-09-08 12:06:23 - Starting prediction on cuda\n",
      "Prediction: 100%|██████████| 2180/2180 [02:26<00:00, 14.87it/s]\n",
      "[DEBUG] 2023-09-08 12:08:50 - Finished prediction\n",
      "[DEBUG] 2023-09-08 12:08:52 - Starting prediction on cuda\n",
      "Prediction: 100%|██████████| 2180/2180 [02:26<00:00, 14.88it/s]\n",
      "[DEBUG] 2023-09-08 12:11:18 - Finished prediction\n",
      "[DEBUG] 2023-09-08 12:11:20 - Starting prediction on cuda\n",
      "Prediction: 100%|██████████| 2180/2180 [02:27<00:00, 14.83it/s]\n",
      "[DEBUG] 2023-09-08 12:13:47 - Finished prediction\n"
     ]
    }
   ],
   "source": [
    "best_auc_model_paths = [\n",
    "  \"c:\\\\Users\\\\negi\\\\Documents\\\\GitHub\\\\tkser\\\\technopro-food-package\\\\notebook\\\\../src\\\\models\\\\SwinV2Model\\\\../../data/models/SwinV2Model_cv\\\\swintransformerv2_20230907233357_epoch_13_auc_0.9851.pth\",\n",
    "  \"c:\\\\Users\\\\negi\\\\Documents\\\\GitHub\\\\tkser\\\\technopro-food-package\\\\notebook\\\\../src\\\\models\\\\SwinV2Model\\\\../../data/models/SwinV2Model_cv\\\\swintransformerv2_20230908014813_epoch_19_auc_0.9908.pth\",\n",
    "  \"c:\\\\Users\\\\negi\\\\Documents\\\\GitHub\\\\tkser\\\\technopro-food-package\\\\notebook\\\\../src\\\\models\\\\SwinV2Model\\\\../../data/models/SwinV2Model_cv\\\\swintransformerv2_20230908040332_epoch_20_auc_0.9838.pth\",\n",
    "  \"c:\\\\Users\\\\negi\\\\Documents\\\\GitHub\\\\tkser\\\\technopro-food-package\\\\notebook\\\\../src\\\\models\\\\SwinV2Model\\\\../../data/models/SwinV2Model_cv\\\\swintransformerv2_20230908061916_epoch_12_auc_0.9838.pth\",\n",
    "  \"c:\\\\Users\\\\negi\\\\Documents\\\\GitHub\\\\tkser\\\\technopro-food-package\\\\notebook\\\\../src\\\\models\\\\SwinV2Model\\\\../../data/models/SwinV2Model_cv\\\\swintransformerv2_20230908083508_epoch_15_auc_0.9872.pth\"\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "sample_submission = pd.read_csv(\"../src/data/input/sample_submit.csv\", header=None, names=['image_name', 'label'])\n",
    "sample_submission.drop('label', axis=1, inplace=True)\n",
    "\n",
    "for fold, best_auc_model_path in enumerate(best_auc_model_paths):\n",
    "  _, y_pred = predict_SwinV2Model(\n",
    "      model_path=best_auc_model_path,\n",
    "      batch_size=1,\n",
    "  )\n",
    "  sample_submission[f'fold_{fold}'] = y_pred\n",
    "\n",
    "sample_submission['label'] = sample_submission.iloc[:, 1:].mean(axis=1)\n",
    "sample_submission.drop(sample_submission.columns[1:-1], axis=1, inplace=True)\n",
    "\n",
    "sample_submission.to_csv('../src/data/output/submit_swintransformerv2_5fold_auc_0.98614.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(loss_history, \"SwinV2Model Loss Avg. 5-Fold CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(auc_history, \"SwinV2Model AUC Avg. 5-Fold CV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
