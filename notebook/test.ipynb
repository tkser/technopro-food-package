{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model(\"swinv2_large_window12to24_192to384\", pretrained=True, num_classes=2)\n",
    "#model.heads.head = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
    "num_count = 0\n",
    "for param in model.parameters():\n",
    "    num_count += 1\n",
    "print(num_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_12): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_13): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_14): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_15): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_16): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_17): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_18): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_19): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_20): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_21): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_22): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_23): EncoderBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import vit_l_16, ViT_L_16_Weights\n",
    "model = vit_l_16(weights=ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "model.heads.head = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repo\\tkser\\technopro-food-package\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vit_base_patch8_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.augreg_in21k',\n",
       " 'vit_base_patch8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.dino',\n",
       " 'vit_base_patch14_dinov2.lvd142m',\n",
       " 'vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.augreg_in1k',\n",
       " 'vit_base_patch16_224.augreg_in21k',\n",
       " 'vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.dino',\n",
       " 'vit_base_patch16_224.mae',\n",
       " 'vit_base_patch16_224.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.sam_in1k',\n",
       " 'vit_base_patch16_224_miil.in21k',\n",
       " 'vit_base_patch16_224_miil.in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.augreg_in1k',\n",
       " 'vit_base_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.datacompxl',\n",
       " 'vit_base_patch16_clip_224.laion2b',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_224.openai',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_rpn_224.sw_in1k',\n",
       " 'vit_base_patch32_224.augreg_in1k',\n",
       " 'vit_base_patch32_224.augreg_in21k',\n",
       " 'vit_base_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_224.sam_in1k',\n",
       " 'vit_base_patch32_384.augreg_in1k',\n",
       " 'vit_base_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_224.openai',\n",
       " 'vit_base_patch32_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_r50_s16_224.orig_in21k',\n",
       " 'vit_base_r50_s16_384.orig_in21k_ft_in1k',\n",
       " 'vit_giant_patch14_clip_224.laion2b',\n",
       " 'vit_giant_patch14_dinov2.lvd142m',\n",
       " 'vit_gigantic_patch14_clip_224.laion2b',\n",
       " 'vit_gigantic_patch16_224_ijepa.in22k',\n",
       " 'vit_huge_patch14_224.mae',\n",
       " 'vit_huge_patch14_224.orig_in21k',\n",
       " 'vit_huge_patch14_224_ijepa.in1k',\n",
       " 'vit_huge_patch14_224_ijepa.in22k',\n",
       " 'vit_huge_patch14_clip_224.laion2b',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch16_448_ijepa.in1k',\n",
       " 'vit_large_patch14_clip_224.datacompxl',\n",
       " 'vit_large_patch14_clip_224.laion2b',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_224.openai',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.openai',\n",
       " 'vit_large_patch14_clip_336.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_dinov2.lvd142m',\n",
       " 'vit_large_patch16_224.augreg_in21k',\n",
       " 'vit_large_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch16_224.mae',\n",
       " 'vit_large_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch32_224.orig_in21k',\n",
       " 'vit_large_patch32_384.orig_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_240.sw_in12k',\n",
       " 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k',\n",
       " 'vit_relpos_base_patch16_224.sw_in1k',\n",
       " 'vit_relpos_base_patch16_clsgap_224.sw_in1k',\n",
       " 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_cls_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_rpn_224.sw_in1k',\n",
       " 'vit_relpos_small_patch16_224.sw_in1k',\n",
       " 'vit_small_patch8_224.dino',\n",
       " 'vit_small_patch14_dinov2.lvd142m',\n",
       " 'vit_small_patch16_224.augreg_in1k',\n",
       " 'vit_small_patch16_224.augreg_in21k',\n",
       " 'vit_small_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch16_224.dino',\n",
       " 'vit_small_patch16_384.augreg_in1k',\n",
       " 'vit_small_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_224.augreg_in21k',\n",
       " 'vit_small_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_srelpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_srelpos_small_patch16_224.sw_in1k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models('vit*', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model(\"vit_large_patch16_384.augreg_in21k_ft_in1k\", pretrained=True, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for param in backbone.parameters():\n",
    "    num += 1\n",
    "    param.requires_grad = False\n",
    "    if num == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (9): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (10): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (11): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (12): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (13): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (14): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (15): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (16): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (17): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (18): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (19): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (20): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (21): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (22): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (23): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (24): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (25): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (26): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1536, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "backbone.head.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight False\n",
      "patch_embed.proj.bias False\n",
      "patch_embed.norm.weight False\n",
      "patch_embed.norm.bias False\n",
      "layers.0.blocks.0.attn.logit_scale False\n",
      "layers.0.blocks.0.attn.q_bias False\n",
      "layers.0.blocks.0.attn.v_bias False\n",
      "layers.0.blocks.0.attn.cpb_mlp.0.weight False\n",
      "layers.0.blocks.0.attn.cpb_mlp.0.bias False\n",
      "layers.0.blocks.0.attn.cpb_mlp.2.weight False\n",
      "layers.0.blocks.0.attn.qkv.weight False\n",
      "layers.0.blocks.0.attn.proj.weight False\n",
      "layers.0.blocks.0.attn.proj.bias False\n",
      "layers.0.blocks.0.norm1.weight False\n",
      "layers.0.blocks.0.norm1.bias False\n",
      "layers.0.blocks.0.mlp.fc1.weight False\n",
      "layers.0.blocks.0.mlp.fc1.bias False\n",
      "layers.0.blocks.0.mlp.fc2.weight False\n",
      "layers.0.blocks.0.mlp.fc2.bias False\n",
      "layers.0.blocks.0.norm2.weight False\n",
      "layers.0.blocks.0.norm2.bias False\n",
      "layers.0.blocks.1.attn.logit_scale False\n",
      "layers.0.blocks.1.attn.q_bias False\n",
      "layers.0.blocks.1.attn.v_bias False\n",
      "layers.0.blocks.1.attn.cpb_mlp.0.weight False\n",
      "layers.0.blocks.1.attn.cpb_mlp.0.bias False\n",
      "layers.0.blocks.1.attn.cpb_mlp.2.weight False\n",
      "layers.0.blocks.1.attn.qkv.weight False\n",
      "layers.0.blocks.1.attn.proj.weight False\n",
      "layers.0.blocks.1.attn.proj.bias False\n",
      "layers.0.blocks.1.norm1.weight False\n",
      "layers.0.blocks.1.norm1.bias False\n",
      "layers.0.blocks.1.mlp.fc1.weight False\n",
      "layers.0.blocks.1.mlp.fc1.bias False\n",
      "layers.0.blocks.1.mlp.fc2.weight False\n",
      "layers.0.blocks.1.mlp.fc2.bias False\n",
      "layers.0.blocks.1.norm2.weight False\n",
      "layers.0.blocks.1.norm2.bias False\n",
      "layers.1.downsample.reduction.weight False\n",
      "layers.1.downsample.norm.weight False\n",
      "layers.1.downsample.norm.bias False\n",
      "layers.1.blocks.0.attn.logit_scale False\n",
      "layers.1.blocks.0.attn.q_bias False\n",
      "layers.1.blocks.0.attn.v_bias False\n",
      "layers.1.blocks.0.attn.cpb_mlp.0.weight False\n",
      "layers.1.blocks.0.attn.cpb_mlp.0.bias False\n",
      "layers.1.blocks.0.attn.cpb_mlp.2.weight False\n",
      "layers.1.blocks.0.attn.qkv.weight False\n",
      "layers.1.blocks.0.attn.proj.weight False\n",
      "layers.1.blocks.0.attn.proj.bias False\n",
      "layers.1.blocks.0.norm1.weight False\n",
      "layers.1.blocks.0.norm1.bias False\n",
      "layers.1.blocks.0.mlp.fc1.weight False\n",
      "layers.1.blocks.0.mlp.fc1.bias False\n",
      "layers.1.blocks.0.mlp.fc2.weight False\n",
      "layers.1.blocks.0.mlp.fc2.bias False\n",
      "layers.1.blocks.0.norm2.weight False\n",
      "layers.1.blocks.0.norm2.bias False\n",
      "layers.1.blocks.1.attn.logit_scale False\n",
      "layers.1.blocks.1.attn.q_bias False\n",
      "layers.1.blocks.1.attn.v_bias False\n",
      "layers.1.blocks.1.attn.cpb_mlp.0.weight False\n",
      "layers.1.blocks.1.attn.cpb_mlp.0.bias False\n",
      "layers.1.blocks.1.attn.cpb_mlp.2.weight False\n",
      "layers.1.blocks.1.attn.qkv.weight False\n",
      "layers.1.blocks.1.attn.proj.weight False\n",
      "layers.1.blocks.1.attn.proj.bias False\n",
      "layers.1.blocks.1.norm1.weight False\n",
      "layers.1.blocks.1.norm1.bias False\n",
      "layers.1.blocks.1.mlp.fc1.weight False\n",
      "layers.1.blocks.1.mlp.fc1.bias False\n",
      "layers.1.blocks.1.mlp.fc2.weight False\n",
      "layers.1.blocks.1.mlp.fc2.bias False\n",
      "layers.1.blocks.1.norm2.weight False\n",
      "layers.1.blocks.1.norm2.bias False\n",
      "layers.2.downsample.reduction.weight False\n",
      "layers.2.downsample.norm.weight False\n",
      "layers.2.downsample.norm.bias False\n",
      "layers.2.blocks.0.attn.logit_scale False\n",
      "layers.2.blocks.0.attn.q_bias False\n",
      "layers.2.blocks.0.attn.v_bias False\n",
      "layers.2.blocks.0.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.0.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.0.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.0.attn.qkv.weight False\n",
      "layers.2.blocks.0.attn.proj.weight False\n",
      "layers.2.blocks.0.attn.proj.bias False\n",
      "layers.2.blocks.0.norm1.weight False\n",
      "layers.2.blocks.0.norm1.bias False\n",
      "layers.2.blocks.0.mlp.fc1.weight False\n",
      "layers.2.blocks.0.mlp.fc1.bias False\n",
      "layers.2.blocks.0.mlp.fc2.weight False\n",
      "layers.2.blocks.0.mlp.fc2.bias False\n",
      "layers.2.blocks.0.norm2.weight False\n",
      "layers.2.blocks.0.norm2.bias False\n",
      "layers.2.blocks.1.attn.logit_scale False\n",
      "layers.2.blocks.1.attn.q_bias False\n",
      "layers.2.blocks.1.attn.v_bias False\n",
      "layers.2.blocks.1.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.1.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.1.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.1.attn.qkv.weight False\n",
      "layers.2.blocks.1.attn.proj.weight False\n",
      "layers.2.blocks.1.attn.proj.bias False\n",
      "layers.2.blocks.1.norm1.weight False\n",
      "layers.2.blocks.1.norm1.bias False\n",
      "layers.2.blocks.1.mlp.fc1.weight False\n",
      "layers.2.blocks.1.mlp.fc1.bias False\n",
      "layers.2.blocks.1.mlp.fc2.weight False\n",
      "layers.2.blocks.1.mlp.fc2.bias False\n",
      "layers.2.blocks.1.norm2.weight False\n",
      "layers.2.blocks.1.norm2.bias False\n",
      "layers.2.blocks.2.attn.logit_scale False\n",
      "layers.2.blocks.2.attn.q_bias False\n",
      "layers.2.blocks.2.attn.v_bias False\n",
      "layers.2.blocks.2.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.2.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.2.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.2.attn.qkv.weight False\n",
      "layers.2.blocks.2.attn.proj.weight False\n",
      "layers.2.blocks.2.attn.proj.bias False\n",
      "layers.2.blocks.2.norm1.weight False\n",
      "layers.2.blocks.2.norm1.bias False\n",
      "layers.2.blocks.2.mlp.fc1.weight False\n",
      "layers.2.blocks.2.mlp.fc1.bias False\n",
      "layers.2.blocks.2.mlp.fc2.weight False\n",
      "layers.2.blocks.2.mlp.fc2.bias False\n",
      "layers.2.blocks.2.norm2.weight False\n",
      "layers.2.blocks.2.norm2.bias False\n",
      "layers.2.blocks.3.attn.logit_scale False\n",
      "layers.2.blocks.3.attn.q_bias False\n",
      "layers.2.blocks.3.attn.v_bias False\n",
      "layers.2.blocks.3.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.3.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.3.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.3.attn.qkv.weight False\n",
      "layers.2.blocks.3.attn.proj.weight False\n",
      "layers.2.blocks.3.attn.proj.bias False\n",
      "layers.2.blocks.3.norm1.weight False\n",
      "layers.2.blocks.3.norm1.bias False\n",
      "layers.2.blocks.3.mlp.fc1.weight False\n",
      "layers.2.blocks.3.mlp.fc1.bias False\n",
      "layers.2.blocks.3.mlp.fc2.weight False\n",
      "layers.2.blocks.3.mlp.fc2.bias False\n",
      "layers.2.blocks.3.norm2.weight False\n",
      "layers.2.blocks.3.norm2.bias False\n",
      "layers.2.blocks.4.attn.logit_scale False\n",
      "layers.2.blocks.4.attn.q_bias False\n",
      "layers.2.blocks.4.attn.v_bias False\n",
      "layers.2.blocks.4.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.4.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.4.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.4.attn.qkv.weight False\n",
      "layers.2.blocks.4.attn.proj.weight False\n",
      "layers.2.blocks.4.attn.proj.bias False\n",
      "layers.2.blocks.4.norm1.weight False\n",
      "layers.2.blocks.4.norm1.bias False\n",
      "layers.2.blocks.4.mlp.fc1.weight False\n",
      "layers.2.blocks.4.mlp.fc1.bias False\n",
      "layers.2.blocks.4.mlp.fc2.weight False\n",
      "layers.2.blocks.4.mlp.fc2.bias False\n",
      "layers.2.blocks.4.norm2.weight False\n",
      "layers.2.blocks.4.norm2.bias False\n",
      "layers.2.blocks.5.attn.logit_scale False\n",
      "layers.2.blocks.5.attn.q_bias False\n",
      "layers.2.blocks.5.attn.v_bias False\n",
      "layers.2.blocks.5.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.5.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.5.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.5.attn.qkv.weight False\n",
      "layers.2.blocks.5.attn.proj.weight False\n",
      "layers.2.blocks.5.attn.proj.bias False\n",
      "layers.2.blocks.5.norm1.weight False\n",
      "layers.2.blocks.5.norm1.bias False\n",
      "layers.2.blocks.5.mlp.fc1.weight False\n",
      "layers.2.blocks.5.mlp.fc1.bias False\n",
      "layers.2.blocks.5.mlp.fc2.weight False\n",
      "layers.2.blocks.5.mlp.fc2.bias False\n",
      "layers.2.blocks.5.norm2.weight False\n",
      "layers.2.blocks.5.norm2.bias False\n",
      "layers.2.blocks.6.attn.logit_scale False\n",
      "layers.2.blocks.6.attn.q_bias False\n",
      "layers.2.blocks.6.attn.v_bias False\n",
      "layers.2.blocks.6.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.6.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.6.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.6.attn.qkv.weight False\n",
      "layers.2.blocks.6.attn.proj.weight False\n",
      "layers.2.blocks.6.attn.proj.bias False\n",
      "layers.2.blocks.6.norm1.weight False\n",
      "layers.2.blocks.6.norm1.bias False\n",
      "layers.2.blocks.6.mlp.fc1.weight False\n",
      "layers.2.blocks.6.mlp.fc1.bias False\n",
      "layers.2.blocks.6.mlp.fc2.weight False\n",
      "layers.2.blocks.6.mlp.fc2.bias False\n",
      "layers.2.blocks.6.norm2.weight False\n",
      "layers.2.blocks.6.norm2.bias False\n",
      "layers.2.blocks.7.attn.logit_scale False\n",
      "layers.2.blocks.7.attn.q_bias False\n",
      "layers.2.blocks.7.attn.v_bias False\n",
      "layers.2.blocks.7.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.7.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.7.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.7.attn.qkv.weight False\n",
      "layers.2.blocks.7.attn.proj.weight False\n",
      "layers.2.blocks.7.attn.proj.bias False\n",
      "layers.2.blocks.7.norm1.weight False\n",
      "layers.2.blocks.7.norm1.bias False\n",
      "layers.2.blocks.7.mlp.fc1.weight False\n",
      "layers.2.blocks.7.mlp.fc1.bias False\n",
      "layers.2.blocks.7.mlp.fc2.weight False\n",
      "layers.2.blocks.7.mlp.fc2.bias False\n",
      "layers.2.blocks.7.norm2.weight False\n",
      "layers.2.blocks.7.norm2.bias False\n",
      "layers.2.blocks.8.attn.logit_scale False\n",
      "layers.2.blocks.8.attn.q_bias False\n",
      "layers.2.blocks.8.attn.v_bias False\n",
      "layers.2.blocks.8.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.8.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.8.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.8.attn.qkv.weight False\n",
      "layers.2.blocks.8.attn.proj.weight False\n",
      "layers.2.blocks.8.attn.proj.bias False\n",
      "layers.2.blocks.8.norm1.weight False\n",
      "layers.2.blocks.8.norm1.bias False\n",
      "layers.2.blocks.8.mlp.fc1.weight False\n",
      "layers.2.blocks.8.mlp.fc1.bias False\n",
      "layers.2.blocks.8.mlp.fc2.weight False\n",
      "layers.2.blocks.8.mlp.fc2.bias False\n",
      "layers.2.blocks.8.norm2.weight False\n",
      "layers.2.blocks.8.norm2.bias False\n",
      "layers.2.blocks.9.attn.logit_scale False\n",
      "layers.2.blocks.9.attn.q_bias False\n",
      "layers.2.blocks.9.attn.v_bias False\n",
      "layers.2.blocks.9.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.9.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.9.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.9.attn.qkv.weight False\n",
      "layers.2.blocks.9.attn.proj.weight False\n",
      "layers.2.blocks.9.attn.proj.bias False\n",
      "layers.2.blocks.9.norm1.weight False\n",
      "layers.2.blocks.9.norm1.bias False\n",
      "layers.2.blocks.9.mlp.fc1.weight False\n",
      "layers.2.blocks.9.mlp.fc1.bias False\n",
      "layers.2.blocks.9.mlp.fc2.weight False\n",
      "layers.2.blocks.9.mlp.fc2.bias False\n",
      "layers.2.blocks.9.norm2.weight False\n",
      "layers.2.blocks.9.norm2.bias False\n",
      "layers.2.blocks.10.attn.logit_scale False\n",
      "layers.2.blocks.10.attn.q_bias False\n",
      "layers.2.blocks.10.attn.v_bias False\n",
      "layers.2.blocks.10.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.10.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.10.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.10.attn.qkv.weight False\n",
      "layers.2.blocks.10.attn.proj.weight False\n",
      "layers.2.blocks.10.attn.proj.bias False\n",
      "layers.2.blocks.10.norm1.weight False\n",
      "layers.2.blocks.10.norm1.bias False\n",
      "layers.2.blocks.10.mlp.fc1.weight False\n",
      "layers.2.blocks.10.mlp.fc1.bias False\n",
      "layers.2.blocks.10.mlp.fc2.weight False\n",
      "layers.2.blocks.10.mlp.fc2.bias False\n",
      "layers.2.blocks.10.norm2.weight False\n",
      "layers.2.blocks.10.norm2.bias False\n",
      "layers.2.blocks.11.attn.logit_scale False\n",
      "layers.2.blocks.11.attn.q_bias False\n",
      "layers.2.blocks.11.attn.v_bias False\n",
      "layers.2.blocks.11.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.11.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.11.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.11.attn.qkv.weight False\n",
      "layers.2.blocks.11.attn.proj.weight False\n",
      "layers.2.blocks.11.attn.proj.bias False\n",
      "layers.2.blocks.11.norm1.weight False\n",
      "layers.2.blocks.11.norm1.bias False\n",
      "layers.2.blocks.11.mlp.fc1.weight False\n",
      "layers.2.blocks.11.mlp.fc1.bias False\n",
      "layers.2.blocks.11.mlp.fc2.weight False\n",
      "layers.2.blocks.11.mlp.fc2.bias False\n",
      "layers.2.blocks.11.norm2.weight False\n",
      "layers.2.blocks.11.norm2.bias False\n",
      "layers.2.blocks.12.attn.logit_scale False\n",
      "layers.2.blocks.12.attn.q_bias False\n",
      "layers.2.blocks.12.attn.v_bias False\n",
      "layers.2.blocks.12.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.12.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.12.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.12.attn.qkv.weight False\n",
      "layers.2.blocks.12.attn.proj.weight False\n",
      "layers.2.blocks.12.attn.proj.bias False\n",
      "layers.2.blocks.12.norm1.weight False\n",
      "layers.2.blocks.12.norm1.bias False\n",
      "layers.2.blocks.12.mlp.fc1.weight False\n",
      "layers.2.blocks.12.mlp.fc1.bias False\n",
      "layers.2.blocks.12.mlp.fc2.weight False\n",
      "layers.2.blocks.12.mlp.fc2.bias False\n",
      "layers.2.blocks.12.norm2.weight False\n",
      "layers.2.blocks.12.norm2.bias False\n",
      "layers.2.blocks.13.attn.logit_scale False\n",
      "layers.2.blocks.13.attn.q_bias False\n",
      "layers.2.blocks.13.attn.v_bias False\n",
      "layers.2.blocks.13.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.13.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.13.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.13.attn.qkv.weight False\n",
      "layers.2.blocks.13.attn.proj.weight False\n",
      "layers.2.blocks.13.attn.proj.bias False\n",
      "layers.2.blocks.13.norm1.weight False\n",
      "layers.2.blocks.13.norm1.bias False\n",
      "layers.2.blocks.13.mlp.fc1.weight False\n",
      "layers.2.blocks.13.mlp.fc1.bias False\n",
      "layers.2.blocks.13.mlp.fc2.weight False\n",
      "layers.2.blocks.13.mlp.fc2.bias False\n",
      "layers.2.blocks.13.norm2.weight False\n",
      "layers.2.blocks.13.norm2.bias False\n",
      "layers.2.blocks.14.attn.logit_scale False\n",
      "layers.2.blocks.14.attn.q_bias False\n",
      "layers.2.blocks.14.attn.v_bias False\n",
      "layers.2.blocks.14.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.14.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.14.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.14.attn.qkv.weight False\n",
      "layers.2.blocks.14.attn.proj.weight False\n",
      "layers.2.blocks.14.attn.proj.bias False\n",
      "layers.2.blocks.14.norm1.weight False\n",
      "layers.2.blocks.14.norm1.bias False\n",
      "layers.2.blocks.14.mlp.fc1.weight False\n",
      "layers.2.blocks.14.mlp.fc1.bias False\n",
      "layers.2.blocks.14.mlp.fc2.weight False\n",
      "layers.2.blocks.14.mlp.fc2.bias False\n",
      "layers.2.blocks.14.norm2.weight False\n",
      "layers.2.blocks.14.norm2.bias False\n",
      "layers.2.blocks.15.attn.logit_scale False\n",
      "layers.2.blocks.15.attn.q_bias False\n",
      "layers.2.blocks.15.attn.v_bias False\n",
      "layers.2.blocks.15.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.15.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.15.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.15.attn.qkv.weight False\n",
      "layers.2.blocks.15.attn.proj.weight False\n",
      "layers.2.blocks.15.attn.proj.bias False\n",
      "layers.2.blocks.15.norm1.weight False\n",
      "layers.2.blocks.15.norm1.bias False\n",
      "layers.2.blocks.15.mlp.fc1.weight False\n",
      "layers.2.blocks.15.mlp.fc1.bias False\n",
      "layers.2.blocks.15.mlp.fc2.weight False\n",
      "layers.2.blocks.15.mlp.fc2.bias False\n",
      "layers.2.blocks.15.norm2.weight False\n",
      "layers.2.blocks.15.norm2.bias False\n",
      "layers.2.blocks.16.attn.logit_scale False\n",
      "layers.2.blocks.16.attn.q_bias False\n",
      "layers.2.blocks.16.attn.v_bias False\n",
      "layers.2.blocks.16.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.16.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.16.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.16.attn.qkv.weight False\n",
      "layers.2.blocks.16.attn.proj.weight False\n",
      "layers.2.blocks.16.attn.proj.bias False\n",
      "layers.2.blocks.16.norm1.weight False\n",
      "layers.2.blocks.16.norm1.bias False\n",
      "layers.2.blocks.16.mlp.fc1.weight False\n",
      "layers.2.blocks.16.mlp.fc1.bias False\n",
      "layers.2.blocks.16.mlp.fc2.weight False\n",
      "layers.2.blocks.16.mlp.fc2.bias False\n",
      "layers.2.blocks.16.norm2.weight False\n",
      "layers.2.blocks.16.norm2.bias False\n",
      "layers.2.blocks.17.attn.logit_scale False\n",
      "layers.2.blocks.17.attn.q_bias False\n",
      "layers.2.blocks.17.attn.v_bias False\n",
      "layers.2.blocks.17.attn.cpb_mlp.0.weight False\n",
      "layers.2.blocks.17.attn.cpb_mlp.0.bias False\n",
      "layers.2.blocks.17.attn.cpb_mlp.2.weight False\n",
      "layers.2.blocks.17.attn.qkv.weight False\n",
      "layers.2.blocks.17.attn.proj.weight False\n",
      "layers.2.blocks.17.attn.proj.bias False\n",
      "layers.2.blocks.17.norm1.weight False\n",
      "layers.2.blocks.17.norm1.bias False\n",
      "layers.2.blocks.17.mlp.fc1.weight False\n",
      "layers.2.blocks.17.mlp.fc1.bias False\n",
      "layers.2.blocks.17.mlp.fc2.weight False\n",
      "layers.2.blocks.17.mlp.fc2.bias False\n",
      "layers.2.blocks.17.norm2.weight False\n",
      "layers.2.blocks.17.norm2.bias False\n",
      "layers.3.downsample.reduction.weight False\n",
      "layers.3.downsample.norm.weight False\n",
      "layers.3.downsample.norm.bias False\n",
      "layers.3.blocks.0.attn.logit_scale False\n",
      "layers.3.blocks.0.attn.q_bias False\n",
      "layers.3.blocks.0.attn.v_bias False\n",
      "layers.3.blocks.0.attn.cpb_mlp.0.weight False\n",
      "layers.3.blocks.0.attn.cpb_mlp.0.bias False\n",
      "layers.3.blocks.0.attn.cpb_mlp.2.weight False\n",
      "layers.3.blocks.0.attn.qkv.weight False\n",
      "layers.3.blocks.0.attn.proj.weight False\n",
      "layers.3.blocks.0.attn.proj.bias False\n",
      "layers.3.blocks.0.norm1.weight False\n",
      "layers.3.blocks.0.norm1.bias False\n",
      "layers.3.blocks.0.mlp.fc1.weight False\n",
      "layers.3.blocks.0.mlp.fc1.bias False\n",
      "layers.3.blocks.0.mlp.fc2.weight False\n",
      "layers.3.blocks.0.mlp.fc2.bias False\n",
      "layers.3.blocks.0.norm2.weight False\n",
      "layers.3.blocks.0.norm2.bias False\n",
      "layers.3.blocks.1.attn.logit_scale False\n",
      "layers.3.blocks.1.attn.q_bias False\n",
      "layers.3.blocks.1.attn.v_bias False\n",
      "layers.3.blocks.1.attn.cpb_mlp.0.weight False\n",
      "layers.3.blocks.1.attn.cpb_mlp.0.bias False\n",
      "layers.3.blocks.1.attn.cpb_mlp.2.weight False\n",
      "layers.3.blocks.1.attn.qkv.weight False\n",
      "layers.3.blocks.1.attn.proj.weight False\n",
      "layers.3.blocks.1.attn.proj.bias False\n",
      "layers.3.blocks.1.norm1.weight False\n",
      "layers.3.blocks.1.norm1.bias False\n",
      "layers.3.blocks.1.mlp.fc1.weight False\n",
      "layers.3.blocks.1.mlp.fc1.bias False\n",
      "layers.3.blocks.1.mlp.fc2.weight False\n",
      "layers.3.blocks.1.mlp.fc2.bias False\n",
      "layers.3.blocks.1.norm2.weight False\n",
      "layers.3.blocks.1.norm2.bias False\n",
      "norm.weight False\n",
      "norm.bias False\n",
      "head.fc.weight True\n",
      "head.fc.bias True\n"
     ]
    }
   ],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "backbone.head.fc = nn.Linear(1000, 2, bias=True)\n",
    "for param in backbone.head.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in backbone.named_parameters():\n",
    "    print(param[0], param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_class = pd.DataFrame(wine.target, columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_cat = pd.concat([wine_df, wine_class], axis=1)\n",
    "wine_cat.drop(wine_cat[wine_cat['class'] == 2].index, inplace=True)\n",
    "wine_data = wine_cat.values[:,:13]\n",
    "wine_target = wine_cat.values[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(wine_data, wine_target, test_size=0.25)\n",
    "train_X = torch.FloatTensor(Train_X)\n",
    "train_Y = torch.LongTensor(Train_Y)\n",
    "test_X = torch.FloatTensor(Test_X)\n",
    "test_Y = torch.LongTensor(Test_Y)\n",
    "train = TensorDataset(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(13, 128)\n",
    "    self.fc2 = nn.Linear(128, 2)\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(0.7221)\n",
      "10 tensor(1.4674)\n",
      "10 tensor(2.1879)\n",
      "10 tensor(2.8625)\n",
      "10 tensor(3.5370)\n",
      "10 tensor(4.2574)\n",
      "10 tensor(4.9546)\n",
      "10 tensor(5.6293)\n",
      "10 tensor(6.2812)\n",
      "10 tensor(6.9322)\n",
      "10 tensor(7.6061)\n",
      "10 tensor(8.2560)\n",
      "10 tensor(8.8562)\n",
      "20 tensor(0.6349)\n",
      "20 tensor(1.3373)\n",
      "20 tensor(2.1414)\n",
      "20 tensor(2.7775)\n",
      "20 tensor(3.4795)\n",
      "20 tensor(4.1814)\n",
      "20 tensor(4.8832)\n",
      "20 tensor(5.5521)\n",
      "20 tensor(6.2210)\n",
      "20 tensor(6.8898)\n",
      "20 tensor(7.6250)\n",
      "20 tensor(8.2284)\n",
      "20 tensor(9.0647)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.7335)\n",
      "30 tensor(1.3713)\n",
      "30 tensor(2.0727)\n",
      "30 tensor(2.7421)\n",
      "30 tensor(3.4434)\n",
      "30 tensor(4.0808)\n",
      "30 tensor(4.7499)\n",
      "30 tensor(5.3212)\n",
      "30 tensor(6.0571)\n",
      "30 tensor(6.7590)\n",
      "30 tensor(7.4278)\n",
      "30 tensor(8.2293)\n",
      "30 tensor(9.0589)\n",
      "40 tensor(0.6412)\n",
      "40 tensor(1.3714)\n",
      "40 tensor(2.0420)\n",
      "40 tensor(2.7126)\n",
      "40 tensor(3.3532)\n",
      "40 tensor(4.1142)\n",
      "40 tensor(4.8437)\n",
      "40 tensor(5.4566)\n",
      "40 tensor(6.1270)\n",
      "40 tensor(6.7674)\n",
      "40 tensor(7.4679)\n",
      "40 tensor(8.2290)\n",
      "40 tensor(9.0472)\n",
      "50 tensor(0.6721)\n",
      "50 tensor(1.3441)\n",
      "50 tensor(1.9890)\n",
      "50 tensor(2.7158)\n",
      "50 tensor(3.3877)\n",
      "50 tensor(4.0594)\n",
      "50 tensor(4.7037)\n",
      "50 tensor(5.4030)\n",
      "50 tensor(6.1302)\n",
      "50 tensor(6.8567)\n",
      "50 tensor(7.5557)\n",
      "50 tensor(8.2546)\n",
      "50 tensor(8.8467)\n",
      "60 tensor(0.7338)\n",
      "60 tensor(1.3395)\n",
      "60 tensor(2.0086)\n",
      "60 tensor(2.6775)\n",
      "60 tensor(3.3793)\n",
      "60 tensor(4.1137)\n",
      "60 tensor(4.7829)\n",
      "60 tensor(5.4196)\n",
      "60 tensor(6.1543)\n",
      "60 tensor(6.8882)\n",
      "60 tensor(7.5256)\n",
      "60 tensor(8.2595)\n",
      "60 tensor(8.8331)\n",
      "70 tensor(0.6362)\n",
      "70 tensor(1.3381)\n",
      "70 tensor(1.9739)\n",
      "70 tensor(2.6425)\n",
      "70 tensor(3.3783)\n",
      "70 tensor(4.1466)\n",
      "70 tensor(4.8158)\n",
      "70 tensor(5.4848)\n",
      "70 tensor(6.2192)\n",
      "70 tensor(6.8884)\n",
      "70 tensor(7.5899)\n",
      "70 tensor(8.2591)\n",
      "70 tensor(8.8312)\n",
      "80 tensor(0.6377)\n",
      "80 tensor(1.3069)\n",
      "80 tensor(2.0084)\n",
      "80 tensor(2.7420)\n",
      "80 tensor(3.3481)\n",
      "80 tensor(4.0821)\n",
      "80 tensor(4.8155)\n",
      "80 tensor(5.4535)\n",
      "80 tensor(6.0908)\n",
      "80 tensor(6.7923)\n",
      "80 tensor(7.5261)\n",
      "80 tensor(8.2274)\n",
      "80 tensor(9.0553)\n",
      "90 tensor(0.6992)\n",
      "90 tensor(1.3709)\n",
      "90 tensor(2.0977)\n",
      "90 tensor(2.7696)\n",
      "90 tensor(3.5232)\n",
      "90 tensor(4.1955)\n",
      "90 tensor(4.8943)\n",
      "90 tensor(5.5665)\n",
      "90 tensor(6.2654)\n",
      "90 tensor(6.9110)\n",
      "90 tensor(7.5829)\n",
      "90 tensor(8.2547)\n",
      "90 tensor(8.8441)\n",
      "100 tensor(0.6705)\n",
      "100 tensor(1.3708)\n",
      "100 tensor(2.0709)\n",
      "100 tensor(2.8005)\n",
      "100 tensor(3.5583)\n",
      "100 tensor(4.2014)\n",
      "100 tensor(4.8724)\n",
      "100 tensor(5.5433)\n",
      "100 tensor(6.2722)\n",
      "100 tensor(6.9147)\n",
      "100 tensor(7.6146)\n",
      "100 tensor(8.2566)\n",
      "100 tensor(8.8389)\n",
      "110 tensor(0.6679)\n",
      "110 tensor(1.4061)\n",
      "110 tensor(2.0047)\n",
      "110 tensor(2.7434)\n",
      "110 tensor(3.4113)\n",
      "110 tensor(4.0792)\n",
      "110 tensor(4.8175)\n",
      "110 tensor(5.5203)\n",
      "110 tensor(6.1192)\n",
      "110 tensor(6.8577)\n",
      "110 tensor(7.5954)\n",
      "110 tensor(8.2636)\n",
      "110 tensor(8.8281)\n",
      "120 tensor(0.6684)\n",
      "120 tensor(1.3028)\n",
      "120 tensor(2.0054)\n",
      "120 tensor(2.7422)\n",
      "120 tensor(3.4107)\n",
      "120 tensor(4.1130)\n",
      "120 tensor(4.7477)\n",
      "120 tensor(5.4160)\n",
      "120 tensor(6.0842)\n",
      "120 tensor(6.7868)\n",
      "120 tensor(7.5924)\n",
      "120 tensor(8.2611)\n",
      "120 tensor(8.8297)\n",
      "130 tensor(0.6406)\n",
      "130 tensor(1.3411)\n",
      "130 tensor(2.0113)\n",
      "130 tensor(2.6815)\n",
      "130 tensor(3.4125)\n",
      "130 tensor(4.0228)\n",
      "130 tensor(4.7543)\n",
      "130 tensor(5.5155)\n",
      "130 tensor(6.1565)\n",
      "130 tensor(6.7668)\n",
      "130 tensor(7.4675)\n",
      "130 tensor(8.2295)\n",
      "130 tensor(9.0493)\n",
      "140 tensor(0.6995)\n",
      "140 tensor(1.3989)\n",
      "140 tensor(2.0704)\n",
      "140 tensor(2.6855)\n",
      "140 tensor(3.4431)\n",
      "140 tensor(4.1145)\n",
      "140 tensor(4.8140)\n",
      "140 tensor(5.4853)\n",
      "140 tensor(6.1849)\n",
      "140 tensor(6.7998)\n",
      "140 tensor(7.5286)\n",
      "140 tensor(8.2283)\n",
      "140 tensor(9.0414)\n",
      "150 tensor(0.7290)\n",
      "150 tensor(1.4287)\n",
      "150 tensor(2.0998)\n",
      "150 tensor(2.7995)\n",
      "150 tensor(3.5562)\n",
      "150 tensor(4.2555)\n",
      "150 tensor(4.8995)\n",
      "150 tensor(5.5708)\n",
      "150 tensor(6.2704)\n",
      "150 tensor(6.9136)\n",
      "150 tensor(7.5560)\n",
      "150 tensor(8.2559)\n",
      "150 tensor(8.8400)\n",
      "160 tensor(0.6148)\n",
      "160 tensor(1.2856)\n",
      "160 tensor(1.9856)\n",
      "160 tensor(2.6855)\n",
      "160 tensor(3.4142)\n",
      "160 tensor(4.1423)\n",
      "160 tensor(4.8417)\n",
      "160 tensor(5.4853)\n",
      "160 tensor(6.2132)\n",
      "160 tensor(6.9405)\n",
      "160 tensor(7.6122)\n",
      "160 tensor(8.2286)\n",
      "160 tensor(9.0419)\n",
      "170 tensor(0.6707)\n",
      "170 tensor(1.3118)\n",
      "170 tensor(2.0421)\n",
      "170 tensor(2.7717)\n",
      "170 tensor(3.4425)\n",
      "170 tensor(4.0841)\n",
      "170 tensor(4.7249)\n",
      "170 tensor(5.4555)\n",
      "170 tensor(6.0963)\n",
      "170 tensor(6.8268)\n",
      "170 tensor(7.5270)\n",
      "170 tensor(8.2271)\n",
      "170 tensor(9.0448)\n",
      "180 tensor(0.7011)\n",
      "180 tensor(1.3707)\n",
      "180 tensor(2.0086)\n",
      "180 tensor(2.7100)\n",
      "180 tensor(3.4431)\n",
      "180 tensor(4.1442)\n",
      "180 tensor(4.8139)\n",
      "180 tensor(5.4520)\n",
      "180 tensor(6.1852)\n",
      "180 tensor(6.8548)\n",
      "180 tensor(7.5875)\n",
      "180 tensor(8.2572)\n",
      "180 tensor(8.8331)\n",
      "190 tensor(0.7331)\n",
      "190 tensor(1.4342)\n",
      "190 tensor(2.1038)\n",
      "190 tensor(2.8049)\n",
      "190 tensor(3.4120)\n",
      "190 tensor(4.1454)\n",
      "190 tensor(4.7834)\n",
      "190 tensor(5.5167)\n",
      "190 tensor(6.2178)\n",
      "190 tensor(6.9817)\n",
      "190 tensor(7.6210)\n",
      "190 tensor(8.2597)\n",
      "190 tensor(8.8346)\n",
      "200 tensor(0.7003)\n",
      "200 tensor(1.4006)\n",
      "200 tensor(2.1008)\n",
      "200 tensor(2.7418)\n",
      "200 tensor(3.4722)\n",
      "200 tensor(4.1428)\n",
      "200 tensor(4.8132)\n",
      "200 tensor(5.5135)\n",
      "200 tensor(6.1542)\n",
      "200 tensor(6.9151)\n",
      "200 tensor(7.5563)\n",
      "200 tensor(8.2566)\n",
      "200 tensor(8.8375)\n",
      "210 tensor(0.6994)\n",
      "210 tensor(1.3430)\n",
      "210 tensor(2.0426)\n",
      "210 tensor(2.7703)\n",
      "210 tensor(3.4418)\n",
      "210 tensor(4.1412)\n",
      "210 tensor(4.8405)\n",
      "210 tensor(5.5675)\n",
      "210 tensor(6.2121)\n",
      "210 tensor(6.9114)\n",
      "210 tensor(7.5830)\n",
      "210 tensor(8.2546)\n",
      "210 tensor(8.8423)\n",
      "220 tensor(0.7012)\n",
      "220 tensor(1.4656)\n",
      "220 tensor(2.2280)\n",
      "220 tensor(2.8382)\n",
      "220 tensor(3.5698)\n",
      "220 tensor(4.1791)\n",
      "220 tensor(4.8800)\n",
      "220 tensor(5.6429)\n",
      "220 tensor(6.2526)\n",
      "220 tensor(6.9534)\n",
      "220 tensor(7.5307)\n",
      "220 tensor(8.2320)\n",
      "220 tensor(9.0605)\n",
      "230 tensor(0.6697)\n",
      "230 tensor(1.4022)\n",
      "230 tensor(2.1030)\n",
      "230 tensor(2.8037)\n",
      "230 tensor(3.4737)\n",
      "230 tensor(4.2052)\n",
      "230 tensor(4.9057)\n",
      "230 tensor(5.5760)\n",
      "230 tensor(6.2157)\n",
      "230 tensor(6.9165)\n",
      "230 tensor(7.5864)\n",
      "230 tensor(8.2255)\n",
      "230 tensor(9.0517)\n",
      "240 tensor(0.6994)\n",
      "240 tensor(1.3709)\n",
      "240 tensor(2.0423)\n",
      "240 tensor(2.7137)\n",
      "240 tensor(3.4132)\n",
      "240 tensor(4.0845)\n",
      "240 tensor(4.7841)\n",
      "240 tensor(5.4836)\n",
      "240 tensor(6.1830)\n",
      "240 tensor(6.8266)\n",
      "240 tensor(7.5545)\n",
      "240 tensor(8.2260)\n",
      "240 tensor(9.0378)\n",
      "250 tensor(0.6708)\n",
      "250 tensor(1.4001)\n",
      "250 tensor(2.1289)\n",
      "250 tensor(2.8569)\n",
      "250 tensor(3.5283)\n",
      "250 tensor(4.1715)\n",
      "250 tensor(4.8425)\n",
      "250 tensor(5.5135)\n",
      "250 tensor(6.2423)\n",
      "250 tensor(6.9420)\n",
      "250 tensor(7.6132)\n",
      "250 tensor(8.2557)\n",
      "250 tensor(8.8394)\n",
      "260 tensor(0.6697)\n",
      "260 tensor(1.3393)\n",
      "260 tensor(1.9772)\n",
      "260 tensor(2.6465)\n",
      "260 tensor(3.3801)\n",
      "260 tensor(4.0496)\n",
      "260 tensor(4.7827)\n",
      "260 tensor(5.5466)\n",
      "260 tensor(6.2166)\n",
      "260 tensor(6.8865)\n",
      "260 tensor(7.5563)\n",
      "260 tensor(8.2572)\n",
      "260 tensor(8.8342)\n",
      "270 tensor(0.6121)\n",
      "270 tensor(1.3727)\n",
      "270 tensor(2.0727)\n",
      "270 tensor(2.8018)\n",
      "270 tensor(3.5015)\n",
      "270 tensor(4.1726)\n",
      "270 tensor(4.8724)\n",
      "270 tensor(5.5434)\n",
      "270 tensor(6.2144)\n",
      "270 tensor(6.8853)\n",
      "270 tensor(7.5561)\n",
      "270 tensor(8.2268)\n",
      "270 tensor(9.0447)\n",
      "280 tensor(0.6985)\n",
      "280 tensor(1.3712)\n",
      "280 tensor(2.0697)\n",
      "280 tensor(2.7424)\n",
      "280 tensor(3.3889)\n",
      "280 tensor(4.0876)\n",
      "280 tensor(4.7599)\n",
      "280 tensor(5.4853)\n",
      "280 tensor(6.1578)\n",
      "280 tensor(6.8827)\n",
      "280 tensor(7.5812)\n",
      "280 tensor(8.2539)\n",
      "280 tensor(8.8488)\n",
      "290 tensor(0.6985)\n",
      "290 tensor(1.3712)\n",
      "290 tensor(2.0957)\n",
      "290 tensor(2.7430)\n",
      "290 tensor(3.4936)\n",
      "290 tensor(4.1413)\n",
      "290 tensor(4.7882)\n",
      "290 tensor(5.5131)\n",
      "290 tensor(6.1599)\n",
      "290 tensor(6.8586)\n",
      "290 tensor(7.5573)\n",
      "290 tensor(8.2558)\n",
      "290 tensor(8.8506)\n",
      "300 tensor(0.7228)\n",
      "300 tensor(1.4449)\n",
      "300 tensor(2.1426)\n",
      "300 tensor(2.8165)\n",
      "300 tensor(3.5142)\n",
      "300 tensor(4.1643)\n",
      "300 tensor(4.8378)\n",
      "300 tensor(5.5113)\n",
      "300 tensor(6.1846)\n",
      "300 tensor(6.8827)\n",
      "300 tensor(7.5312)\n",
      "300 tensor(8.2294)\n",
      "300 tensor(9.0282)\n",
      "310 tensor(0.7262)\n",
      "310 tensor(1.4251)\n",
      "310 tensor(2.1505)\n",
      "310 tensor(2.8491)\n",
      "310 tensor(3.5476)\n",
      "310 tensor(4.2202)\n",
      "310 tensor(4.8667)\n",
      "310 tensor(5.5654)\n",
      "310 tensor(6.2377)\n",
      "310 tensor(6.9365)\n",
      "310 tensor(7.6353)\n",
      "310 tensor(8.2549)\n",
      "310 tensor(8.8453)\n",
      "320 tensor(0.7653)\n",
      "320 tensor(1.4662)\n",
      "320 tensor(2.1671)\n",
      "320 tensor(2.8060)\n",
      "320 tensor(3.4757)\n",
      "320 tensor(4.1452)\n",
      "320 tensor(4.8464)\n",
      "320 tensor(5.6105)\n",
      "320 tensor(6.2804)\n",
      "320 tensor(6.8885)\n",
      "320 tensor(7.5580)\n",
      "320 tensor(8.2274)\n",
      "320 tensor(9.0567)\n",
      "330 tensor(0.6355)\n",
      "330 tensor(1.2366)\n",
      "330 tensor(1.8702)\n",
      "330 tensor(2.6080)\n",
      "330 tensor(3.2418)\n",
      "330 tensor(4.0144)\n",
      "330 tensor(4.6486)\n",
      "330 tensor(5.3859)\n",
      "330 tensor(6.0201)\n",
      "330 tensor(6.7228)\n",
      "330 tensor(7.4253)\n",
      "330 tensor(8.2304)\n",
      "330 tensor(9.0645)\n",
      "340 tensor(0.7004)\n",
      "340 tensor(1.3108)\n",
      "340 tensor(1.9500)\n",
      "340 tensor(2.6509)\n",
      "340 tensor(3.3518)\n",
      "340 tensor(4.1144)\n",
      "340 tensor(4.7846)\n",
      "340 tensor(5.4851)\n",
      "340 tensor(6.1855)\n",
      "340 tensor(6.8258)\n",
      "340 tensor(7.5569)\n",
      "340 tensor(8.2572)\n",
      "340 tensor(8.8380)\n",
      "350 tensor(0.6687)\n",
      "350 tensor(1.3373)\n",
      "350 tensor(1.9387)\n",
      "350 tensor(2.6069)\n",
      "350 tensor(3.4130)\n",
      "350 tensor(4.0817)\n",
      "350 tensor(4.8174)\n",
      "350 tensor(5.5192)\n",
      "350 tensor(6.2210)\n",
      "350 tensor(6.8900)\n",
      "350 tensor(7.5260)\n",
      "350 tensor(8.2280)\n",
      "350 tensor(9.0626)\n",
      "360 tensor(0.6700)\n",
      "360 tensor(1.3707)\n",
      "360 tensor(2.1019)\n",
      "360 tensor(2.8022)\n",
      "360 tensor(3.5025)\n",
      "360 tensor(4.2028)\n",
      "360 tensor(4.8733)\n",
      "360 tensor(5.5139)\n",
      "360 tensor(6.2749)\n",
      "360 tensor(6.8570)\n",
      "360 tensor(7.5576)\n",
      "360 tensor(8.2582)\n",
      "360 tensor(8.8374)\n",
      "370 tensor(0.6719)\n",
      "370 tensor(1.3710)\n",
      "370 tensor(2.0157)\n",
      "370 tensor(2.6597)\n",
      "370 tensor(3.3592)\n",
      "370 tensor(4.0586)\n",
      "370 tensor(4.8414)\n",
      "370 tensor(5.4600)\n",
      "370 tensor(6.1041)\n",
      "370 tensor(6.8035)\n",
      "370 tensor(7.5308)\n",
      "370 tensor(8.2300)\n",
      "370 tensor(9.0384)\n",
      "380 tensor(0.6710)\n",
      "380 tensor(1.3419)\n",
      "380 tensor(2.0127)\n",
      "380 tensor(2.7127)\n",
      "380 tensor(3.3544)\n",
      "380 tensor(3.9658)\n",
      "380 tensor(4.7270)\n",
      "380 tensor(5.4568)\n",
      "380 tensor(6.0984)\n",
      "380 tensor(6.8282)\n",
      "380 tensor(7.5281)\n",
      "380 tensor(8.2280)\n",
      "380 tensor(9.0433)\n",
      "390 tensor(0.7217)\n",
      "390 tensor(1.3722)\n",
      "390 tensor(2.0459)\n",
      "390 tensor(2.7438)\n",
      "390 tensor(3.3692)\n",
      "390 tensor(4.0673)\n",
      "390 tensor(4.7157)\n",
      "390 tensor(5.4140)\n",
      "390 tensor(6.1122)\n",
      "390 tensor(6.8104)\n",
      "390 tensor(7.4836)\n",
      "390 tensor(8.2320)\n",
      "390 tensor(9.0274)\n",
      "400 tensor(0.7540)\n",
      "400 tensor(1.4262)\n",
      "400 tensor(2.1519)\n",
      "400 tensor(2.8507)\n",
      "400 tensor(3.5493)\n",
      "400 tensor(4.1956)\n",
      "400 tensor(4.8412)\n",
      "400 tensor(5.4859)\n",
      "400 tensor(6.1852)\n",
      "400 tensor(6.8843)\n",
      "400 tensor(7.5835)\n",
      "400 tensor(8.2553)\n",
      "400 tensor(8.8449)\n",
      "410 tensor(0.7007)\n",
      "410 tensor(1.4319)\n",
      "410 tensor(2.1925)\n",
      "410 tensor(2.8632)\n",
      "410 tensor(3.5337)\n",
      "410 tensor(4.2339)\n",
      "410 tensor(4.9340)\n",
      "410 tensor(5.5753)\n",
      "410 tensor(6.2457)\n",
      "410 tensor(6.9160)\n",
      "410 tensor(7.5259)\n",
      "410 tensor(8.2267)\n",
      "410 tensor(9.0507)\n",
      "420 tensor(0.6358)\n",
      "420 tensor(1.3045)\n",
      "420 tensor(1.9394)\n",
      "420 tensor(2.6418)\n",
      "420 tensor(3.3102)\n",
      "420 tensor(4.0126)\n",
      "420 tensor(4.7826)\n",
      "420 tensor(5.3853)\n",
      "420 tensor(6.0537)\n",
      "420 tensor(6.7901)\n",
      "420 tensor(7.4922)\n",
      "420 tensor(8.2609)\n",
      "420 tensor(8.8323)\n",
      "430 tensor(0.6381)\n",
      "430 tensor(1.3074)\n",
      "430 tensor(2.0088)\n",
      "430 tensor(2.7420)\n",
      "430 tensor(3.4431)\n",
      "430 tensor(4.1127)\n",
      "430 tensor(4.8768)\n",
      "430 tensor(5.6082)\n",
      "430 tensor(6.2784)\n",
      "430 tensor(6.9181)\n",
      "430 tensor(7.6189)\n",
      "430 tensor(8.2581)\n",
      "430 tensor(8.8342)\n",
      "440 tensor(0.7005)\n",
      "440 tensor(1.4311)\n",
      "440 tensor(2.0719)\n",
      "440 tensor(2.8025)\n",
      "440 tensor(3.4432)\n",
      "440 tensor(4.1436)\n",
      "440 tensor(4.7839)\n",
      "440 tensor(5.5150)\n",
      "440 tensor(6.2455)\n",
      "440 tensor(6.9160)\n",
      "440 tensor(7.5864)\n",
      "440 tensor(8.2568)\n",
      "440 tensor(8.8367)\n",
      "450 tensor(0.7582)\n",
      "450 tensor(1.3727)\n",
      "450 tensor(2.1018)\n",
      "450 tensor(2.7441)\n",
      "450 tensor(3.3857)\n",
      "450 tensor(4.0858)\n",
      "450 tensor(4.7564)\n",
      "450 tensor(5.4566)\n",
      "450 tensor(6.1272)\n",
      "450 tensor(6.8274)\n",
      "450 tensor(7.5275)\n",
      "450 tensor(8.2569)\n",
      "450 tensor(8.8410)\n",
      "460 tensor(0.6690)\n",
      "460 tensor(1.4036)\n",
      "460 tensor(2.1051)\n",
      "460 tensor(2.8388)\n",
      "460 tensor(3.5400)\n",
      "460 tensor(4.2727)\n",
      "460 tensor(4.9113)\n",
      "460 tensor(5.6124)\n",
      "460 tensor(6.2506)\n",
      "460 tensor(6.9837)\n",
      "460 tensor(7.6219)\n",
      "460 tensor(8.2594)\n",
      "460 tensor(8.8314)\n",
      "470 tensor(0.7048)\n",
      "470 tensor(1.4095)\n",
      "470 tensor(2.1518)\n",
      "470 tensor(2.7441)\n",
      "470 tensor(3.3726)\n",
      "470 tensor(4.0391)\n",
      "470 tensor(4.7055)\n",
      "470 tensor(5.4106)\n",
      "470 tensor(6.0385)\n",
      "470 tensor(6.7437)\n",
      "470 tensor(7.4877)\n",
      "470 tensor(8.2692)\n",
      "470 tensor(8.8235)\n",
      "480 tensor(0.6321)\n",
      "480 tensor(1.3356)\n",
      "480 tensor(2.0748)\n",
      "480 tensor(2.7426)\n",
      "480 tensor(3.3748)\n",
      "480 tensor(4.0782)\n",
      "480 tensor(4.7816)\n",
      "480 tensor(5.4492)\n",
      "480 tensor(6.1169)\n",
      "480 tensor(6.8561)\n",
      "480 tensor(7.5238)\n",
      "480 tensor(8.2625)\n",
      "480 tensor(8.8255)\n",
      "490 tensor(0.5913)\n",
      "490 tensor(1.3347)\n",
      "490 tensor(2.0394)\n",
      "490 tensor(2.7061)\n",
      "490 tensor(3.3728)\n",
      "490 tensor(4.1155)\n",
      "490 tensor(4.7824)\n",
      "490 tensor(5.4868)\n",
      "490 tensor(6.1537)\n",
      "490 tensor(6.8205)\n",
      "490 tensor(7.5250)\n",
      "490 tensor(8.2294)\n",
      "490 tensor(9.0830)\n",
      "500 tensor(0.6699)\n",
      "500 tensor(1.3707)\n",
      "500 tensor(2.0714)\n",
      "500 tensor(2.8332)\n",
      "500 tensor(3.5037)\n",
      "500 tensor(4.2040)\n",
      "500 tensor(4.8744)\n",
      "500 tensor(5.5147)\n",
      "500 tensor(6.1848)\n",
      "500 tensor(6.8242)\n",
      "500 tensor(7.4628)\n",
      "500 tensor(8.2587)\n",
      "500 tensor(8.8373)\n",
      "510 tensor(0.6702)\n",
      "510 tensor(1.3403)\n",
      "510 tensor(2.0409)\n",
      "510 tensor(2.7415)\n",
      "510 tensor(3.3813)\n",
      "510 tensor(4.0512)\n",
      "510 tensor(4.7830)\n",
      "510 tensor(5.4835)\n",
      "510 tensor(6.1233)\n",
      "510 tensor(6.7932)\n",
      "510 tensor(7.4631)\n",
      "510 tensor(8.2262)\n",
      "510 tensor(9.0482)\n",
      "520 tensor(0.6989)\n",
      "520 tensor(1.3443)\n",
      "520 tensor(2.0433)\n",
      "520 tensor(2.7966)\n",
      "520 tensor(3.4689)\n",
      "520 tensor(4.2208)\n",
      "520 tensor(4.9192)\n",
      "520 tensor(5.5920)\n",
      "520 tensor(6.2646)\n",
      "520 tensor(6.9111)\n",
      "520 tensor(7.5569)\n",
      "520 tensor(8.2558)\n",
      "520 tensor(8.8473)\n",
      "530 tensor(0.6478)\n",
      "530 tensor(1.3462)\n",
      "530 tensor(2.0446)\n",
      "530 tensor(2.7429)\n",
      "530 tensor(3.3906)\n",
      "530 tensor(4.0890)\n",
      "530 tensor(4.7618)\n",
      "530 tensor(5.4085)\n",
      "530 tensor(6.1072)\n",
      "530 tensor(6.7796)\n",
      "530 tensor(7.5312)\n",
      "530 tensor(8.2552)\n",
      "530 tensor(8.8526)\n",
      "540 tensor(0.6992)\n",
      "540 tensor(1.4531)\n",
      "540 tensor(2.0986)\n",
      "540 tensor(2.7434)\n",
      "540 tensor(3.4703)\n",
      "540 tensor(4.1422)\n",
      "540 tensor(4.8139)\n",
      "540 tensor(5.5406)\n",
      "540 tensor(6.2126)\n",
      "540 tensor(6.9389)\n",
      "540 tensor(7.5574)\n",
      "540 tensor(8.2566)\n",
      "540 tensor(8.8460)\n",
      "550 tensor(0.7252)\n",
      "550 tensor(1.4237)\n",
      "550 tensor(2.0964)\n",
      "550 tensor(2.7949)\n",
      "550 tensor(3.5194)\n",
      "550 tensor(4.1412)\n",
      "550 tensor(4.8136)\n",
      "550 tensor(5.5124)\n",
      "550 tensor(6.1847)\n",
      "550 tensor(6.8569)\n",
      "550 tensor(7.6092)\n",
      "550 tensor(8.2299)\n",
      "550 tensor(9.0358)\n",
      "560 tensor(0.6728)\n",
      "560 tensor(1.3970)\n",
      "560 tensor(2.1207)\n",
      "560 tensor(2.7939)\n",
      "560 tensor(3.4669)\n",
      "560 tensor(4.0893)\n",
      "560 tensor(4.7356)\n",
      "560 tensor(5.4611)\n",
      "560 tensor(6.1335)\n",
      "560 tensor(6.8586)\n",
      "560 tensor(7.5571)\n",
      "560 tensor(8.2297)\n",
      "560 tensor(9.0321)\n",
      "570 tensor(0.6988)\n",
      "570 tensor(1.3182)\n",
      "570 tensor(1.9901)\n",
      "570 tensor(2.6618)\n",
      "570 tensor(3.3059)\n",
      "570 tensor(4.0614)\n",
      "570 tensor(4.7605)\n",
      "570 tensor(5.3781)\n",
      "570 tensor(6.0495)\n",
      "570 tensor(6.8053)\n",
      "570 tensor(7.5044)\n",
      "570 tensor(8.2307)\n",
      "570 tensor(9.0367)\n",
      "580 tensor(0.6720)\n",
      "580 tensor(1.3710)\n",
      "580 tensor(1.9890)\n",
      "580 tensor(2.7439)\n",
      "580 tensor(3.4699)\n",
      "580 tensor(4.1687)\n",
      "580 tensor(4.8410)\n",
      "580 tensor(5.5132)\n",
      "580 tensor(6.1316)\n",
      "580 tensor(6.8585)\n",
      "580 tensor(7.5575)\n",
      "580 tensor(8.2295)\n",
      "580 tensor(9.0374)\n",
      "590 tensor(0.6704)\n",
      "590 tensor(1.3108)\n",
      "590 tensor(2.0418)\n",
      "590 tensor(2.7721)\n",
      "590 tensor(3.4427)\n",
      "590 tensor(4.1429)\n",
      "590 tensor(4.8725)\n",
      "590 tensor(5.5724)\n",
      "590 tensor(6.2433)\n",
      "590 tensor(6.8850)\n",
      "590 tensor(7.5556)\n",
      "590 tensor(8.2260)\n",
      "590 tensor(9.0459)\n",
      "600 tensor(0.7285)\n",
      "600 tensor(1.4280)\n",
      "600 tensor(2.0993)\n",
      "600 tensor(2.7423)\n",
      "600 tensor(3.4132)\n",
      "600 tensor(4.0841)\n",
      "600 tensor(4.8132)\n",
      "600 tensor(5.4555)\n",
      "600 tensor(6.1263)\n",
      "600 tensor(6.8850)\n",
      "600 tensor(7.5561)\n",
      "600 tensor(8.2558)\n",
      "600 tensor(8.8411)\n",
      "610 tensor(0.7017)\n",
      "610 tensor(1.4359)\n",
      "610 tensor(2.1373)\n",
      "610 tensor(2.8705)\n",
      "610 tensor(3.5086)\n",
      "610 tensor(4.2099)\n",
      "610 tensor(4.9111)\n",
      "610 tensor(5.6439)\n",
      "610 tensor(6.3136)\n",
      "610 tensor(6.9520)\n",
      "610 tensor(7.6532)\n",
      "610 tensor(8.2594)\n",
      "610 tensor(8.8313)\n",
      "620 tensor(0.7017)\n",
      "620 tensor(1.4034)\n",
      "620 tensor(2.0400)\n",
      "620 tensor(2.7418)\n",
      "620 tensor(3.4435)\n",
      "620 tensor(4.0799)\n",
      "620 tensor(4.7818)\n",
      "620 tensor(5.4507)\n",
      "620 tensor(6.1525)\n",
      "620 tensor(6.8543)\n",
      "620 tensor(7.5233)\n",
      "620 tensor(8.2579)\n",
      "620 tensor(8.8299)\n",
      "630 tensor(0.7298)\n",
      "630 tensor(1.4589)\n",
      "630 tensor(2.1873)\n",
      "630 tensor(2.8586)\n",
      "630 tensor(3.5582)\n",
      "630 tensor(4.1729)\n",
      "630 tensor(4.9018)\n",
      "630 tensor(5.6015)\n",
      "630 tensor(6.2442)\n",
      "630 tensor(6.9440)\n",
      "630 tensor(7.5574)\n",
      "630 tensor(8.2279)\n",
      "630 tensor(9.0472)\n",
      "640 tensor(0.7002)\n",
      "640 tensor(1.2819)\n",
      "640 tensor(1.9211)\n",
      "640 tensor(2.6221)\n",
      "640 tensor(3.3539)\n",
      "640 tensor(3.9934)\n",
      "640 tensor(4.7563)\n",
      "640 tensor(5.4265)\n",
      "640 tensor(6.0966)\n",
      "640 tensor(6.8584)\n",
      "640 tensor(7.5288)\n",
      "640 tensor(8.2291)\n",
      "640 tensor(9.0487)\n",
      "650 tensor(0.6715)\n",
      "650 tensor(1.4269)\n",
      "650 tensor(2.0987)\n",
      "650 tensor(2.7705)\n",
      "650 tensor(3.4697)\n",
      "650 tensor(4.1414)\n",
      "650 tensor(4.8130)\n",
      "650 tensor(5.5123)\n",
      "650 tensor(6.2116)\n",
      "650 tensor(6.8558)\n",
      "650 tensor(7.5832)\n",
      "650 tensor(8.2548)\n",
      "650 tensor(8.8432)\n",
      "660 tensor(0.6989)\n",
      "660 tensor(1.3443)\n",
      "660 tensor(2.0161)\n",
      "660 tensor(2.7152)\n",
      "660 tensor(3.4143)\n",
      "660 tensor(4.0591)\n",
      "660 tensor(4.7860)\n",
      "660 tensor(5.5122)\n",
      "660 tensor(6.2111)\n",
      "660 tensor(6.9099)\n",
      "660 tensor(7.5821)\n",
      "660 tensor(8.2543)\n",
      "660 tensor(8.8458)\n",
      "670 tensor(0.7325)\n",
      "670 tensor(1.4023)\n",
      "670 tensor(2.1344)\n",
      "670 tensor(2.8350)\n",
      "670 tensor(3.5051)\n",
      "670 tensor(4.1751)\n",
      "670 tensor(4.8758)\n",
      "670 tensor(5.4844)\n",
      "670 tensor(6.1854)\n",
      "670 tensor(6.9177)\n",
      "670 tensor(7.5876)\n",
      "670 tensor(8.2574)\n",
      "670 tensor(8.8336)\n",
      "680 tensor(0.7012)\n",
      "680 tensor(1.4022)\n",
      "680 tensor(2.0719)\n",
      "680 tensor(2.7729)\n",
      "680 tensor(3.4112)\n",
      "680 tensor(4.0806)\n",
      "680 tensor(4.7499)\n",
      "680 tensor(5.3549)\n",
      "680 tensor(6.0567)\n",
      "680 tensor(6.7911)\n",
      "680 tensor(7.4926)\n",
      "680 tensor(8.2261)\n",
      "680 tensor(9.0538)\n",
      "690 tensor(0.7012)\n",
      "690 tensor(1.3392)\n",
      "690 tensor(2.0725)\n",
      "690 tensor(2.8051)\n",
      "690 tensor(3.4749)\n",
      "690 tensor(4.1758)\n",
      "690 tensor(4.8767)\n",
      "690 tensor(5.5465)\n",
      "690 tensor(6.2474)\n",
      "690 tensor(6.9482)\n",
      "690 tensor(7.5566)\n",
      "690 tensor(8.2261)\n",
      "690 tensor(9.0541)\n",
      "700 tensor(0.6446)\n",
      "700 tensor(1.3716)\n",
      "700 tensor(2.0707)\n",
      "700 tensor(2.7697)\n",
      "700 tensor(3.4417)\n",
      "700 tensor(4.1407)\n",
      "700 tensor(4.8127)\n",
      "700 tensor(5.4845)\n",
      "700 tensor(6.1837)\n",
      "700 tensor(6.8555)\n",
      "700 tensor(7.5821)\n",
      "700 tensor(8.2540)\n",
      "700 tensor(8.8444)\n",
      "710 tensor(0.7011)\n",
      "710 tensor(1.3707)\n",
      "710 tensor(2.0718)\n",
      "710 tensor(2.7729)\n",
      "710 tensor(3.4112)\n",
      "710 tensor(4.0807)\n",
      "710 tensor(4.7820)\n",
      "710 tensor(5.4514)\n",
      "710 tensor(6.1846)\n",
      "710 tensor(6.8542)\n",
      "710 tensor(7.4921)\n",
      "710 tensor(8.2255)\n",
      "710 tensor(9.0529)\n",
      "720 tensor(0.6717)\n",
      "720 tensor(1.4539)\n",
      "720 tensor(2.1793)\n",
      "720 tensor(2.8779)\n",
      "720 tensor(3.4984)\n",
      "720 tensor(4.1705)\n",
      "720 tensor(4.8425)\n",
      "720 tensor(5.5143)\n",
      "720 tensor(6.2408)\n",
      "720 tensor(6.8859)\n",
      "720 tensor(7.5851)\n",
      "720 tensor(8.2569)\n",
      "720 tensor(8.8463)\n",
      "730 tensor(0.6432)\n",
      "730 tensor(1.3429)\n",
      "730 tensor(2.0425)\n",
      "730 tensor(2.7988)\n",
      "730 tensor(3.4704)\n",
      "730 tensor(4.1697)\n",
      "730 tensor(4.8137)\n",
      "730 tensor(5.4569)\n",
      "730 tensor(6.1852)\n",
      "730 tensor(6.8565)\n",
      "730 tensor(7.5278)\n",
      "730 tensor(8.2559)\n",
      "730 tensor(8.8432)\n",
      "740 tensor(0.6370)\n",
      "740 tensor(1.2733)\n",
      "740 tensor(2.0084)\n",
      "740 tensor(2.6773)\n",
      "740 tensor(3.3134)\n",
      "740 tensor(4.0487)\n",
      "740 tensor(4.7832)\n",
      "740 tensor(5.5494)\n",
      "740 tensor(6.1874)\n",
      "740 tensor(6.8887)\n",
      "740 tensor(7.4944)\n",
      "740 tensor(8.2287)\n",
      "740 tensor(9.0587)\n",
      "750 tensor(0.6716)\n",
      "750 tensor(1.3431)\n",
      "750 tensor(2.0145)\n",
      "750 tensor(2.7140)\n",
      "750 tensor(3.3853)\n",
      "750 tensor(4.0849)\n",
      "750 tensor(4.7843)\n",
      "750 tensor(5.4557)\n",
      "750 tensor(6.2114)\n",
      "750 tensor(6.8558)\n",
      "750 tensor(7.5830)\n",
      "750 tensor(8.2548)\n",
      "750 tensor(8.8437)\n",
      "760 tensor(0.7358)\n",
      "760 tensor(1.4377)\n",
      "760 tensor(2.1065)\n",
      "760 tensor(2.7422)\n",
      "760 tensor(3.4443)\n",
      "760 tensor(4.1464)\n",
      "760 tensor(4.8483)\n",
      "760 tensor(5.5171)\n",
      "760 tensor(6.1859)\n",
      "760 tensor(6.8545)\n",
      "760 tensor(7.5566)\n",
      "760 tensor(8.2253)\n",
      "760 tensor(9.0614)\n",
      "770 tensor(0.7286)\n",
      "770 tensor(1.3998)\n",
      "770 tensor(2.1564)\n",
      "770 tensor(2.8280)\n",
      "770 tensor(3.5274)\n",
      "770 tensor(4.1155)\n",
      "770 tensor(4.8731)\n",
      "770 tensor(5.5445)\n",
      "770 tensor(6.2157)\n",
      "770 tensor(6.8299)\n",
      "770 tensor(7.5592)\n",
      "770 tensor(8.2590)\n",
      "770 tensor(8.8440)\n",
      "780 tensor(0.7009)\n",
      "780 tensor(1.3398)\n",
      "780 tensor(2.0094)\n",
      "780 tensor(2.7105)\n",
      "780 tensor(3.4429)\n",
      "780 tensor(4.0818)\n",
      "780 tensor(4.7828)\n",
      "780 tensor(5.4525)\n",
      "780 tensor(6.1535)\n",
      "780 tensor(6.8232)\n",
      "780 tensor(7.5871)\n",
      "780 tensor(8.2571)\n",
      "780 tensor(8.8346)\n",
      "790 tensor(0.6055)\n",
      "790 tensor(1.3072)\n",
      "790 tensor(1.9763)\n",
      "790 tensor(2.6453)\n",
      "790 tensor(3.3142)\n",
      "790 tensor(4.0820)\n",
      "790 tensor(4.7512)\n",
      "790 tensor(5.4203)\n",
      "790 tensor(6.1219)\n",
      "790 tensor(6.8557)\n",
      "790 tensor(7.5570)\n",
      "790 tensor(8.2264)\n",
      "790 tensor(9.0551)\n",
      "800 tensor(0.7003)\n",
      "800 tensor(1.3707)\n",
      "800 tensor(2.0710)\n",
      "800 tensor(2.7713)\n",
      "800 tensor(3.3825)\n",
      "800 tensor(4.1135)\n",
      "800 tensor(4.7838)\n",
      "800 tensor(5.5143)\n",
      "800 tensor(6.2442)\n",
      "800 tensor(6.9441)\n",
      "800 tensor(7.5859)\n",
      "800 tensor(8.2565)\n",
      "800 tensor(8.8379)\n",
      "810 tensor(0.6699)\n",
      "810 tensor(1.3707)\n",
      "810 tensor(2.1641)\n",
      "810 tensor(2.8047)\n",
      "810 tensor(3.4447)\n",
      "810 tensor(4.1146)\n",
      "810 tensor(4.8463)\n",
      "810 tensor(5.5468)\n",
      "810 tensor(6.1262)\n",
      "810 tensor(6.8586)\n",
      "810 tensor(7.5284)\n",
      "810 tensor(8.2603)\n",
      "810 tensor(8.8386)\n",
      "820 tensor(0.7002)\n",
      "820 tensor(1.4300)\n",
      "820 tensor(2.1300)\n",
      "820 tensor(2.8589)\n",
      "820 tensor(3.5586)\n",
      "820 tensor(4.2013)\n",
      "820 tensor(4.9011)\n",
      "820 tensor(5.5721)\n",
      "820 tensor(6.3009)\n",
      "820 tensor(6.9435)\n",
      "820 tensor(7.6434)\n",
      "820 tensor(8.2567)\n",
      "820 tensor(8.8383)\n",
      "830 tensor(0.6993)\n",
      "830 tensor(1.4536)\n",
      "830 tensor(2.1257)\n",
      "830 tensor(2.7977)\n",
      "830 tensor(3.4424)\n",
      "830 tensor(4.1416)\n",
      "830 tensor(4.7583)\n",
      "830 tensor(5.4862)\n",
      "830 tensor(6.1020)\n",
      "830 tensor(6.8303)\n",
      "830 tensor(7.5016)\n",
      "830 tensor(8.2295)\n",
      "830 tensor(9.0405)\n",
      "840 tensor(0.7284)\n",
      "840 tensor(1.4279)\n",
      "840 tensor(2.0712)\n",
      "840 tensor(2.6852)\n",
      "840 tensor(3.3852)\n",
      "840 tensor(4.0559)\n",
      "840 tensor(4.8441)\n",
      "840 tensor(5.5153)\n",
      "840 tensor(6.1865)\n",
      "840 tensor(6.8862)\n",
      "840 tensor(7.6143)\n",
      "840 tensor(8.2576)\n",
      "840 tensor(8.8432)\n",
      "850 tensor(0.6453)\n",
      "850 tensor(1.3171)\n",
      "850 tensor(2.0163)\n",
      "850 tensor(2.6607)\n",
      "850 tensor(3.3601)\n",
      "850 tensor(3.9763)\n",
      "850 tensor(4.6190)\n",
      "850 tensor(5.3767)\n",
      "850 tensor(6.0762)\n",
      "850 tensor(6.7195)\n",
      "850 tensor(7.5048)\n",
      "850 tensor(8.2589)\n",
      "850 tensor(8.8509)\n",
      "860 tensor(0.5980)\n",
      "860 tensor(1.2656)\n",
      "860 tensor(1.9690)\n",
      "860 tensor(2.6366)\n",
      "860 tensor(3.3758)\n",
      "860 tensor(4.0789)\n",
      "860 tensor(4.7468)\n",
      "860 tensor(5.4145)\n",
      "860 tensor(6.1886)\n",
      "860 tensor(6.8220)\n",
      "860 tensor(7.5601)\n",
      "860 tensor(8.2628)\n",
      "860 tensor(8.8276)\n",
      "870 tensor(0.6403)\n",
      "870 tensor(1.3409)\n",
      "870 tensor(2.0414)\n",
      "870 tensor(2.7720)\n",
      "870 tensor(3.4127)\n",
      "870 tensor(4.1735)\n",
      "870 tensor(4.8736)\n",
      "870 tensor(5.5150)\n",
      "870 tensor(6.1557)\n",
      "870 tensor(6.9166)\n",
      "870 tensor(7.5873)\n",
      "870 tensor(8.2578)\n",
      "870 tensor(8.8389)\n",
      "880 tensor(0.7001)\n",
      "880 tensor(1.4002)\n",
      "880 tensor(2.0416)\n",
      "880 tensor(2.7716)\n",
      "880 tensor(3.4423)\n",
      "880 tensor(4.1129)\n",
      "880 tensor(4.8131)\n",
      "880 tensor(5.4836)\n",
      "880 tensor(6.1245)\n",
      "880 tensor(6.7947)\n",
      "880 tensor(7.5255)\n",
      "880 tensor(8.2258)\n",
      "880 tensor(9.0448)\n",
      "890 tensor(0.6979)\n",
      "890 tensor(1.3472)\n",
      "890 tensor(2.0452)\n",
      "890 tensor(2.7433)\n",
      "890 tensor(3.4412)\n",
      "890 tensor(4.0659)\n",
      "890 tensor(4.7893)\n",
      "890 tensor(5.5122)\n",
      "890 tensor(6.2344)\n",
      "890 tensor(6.9561)\n",
      "890 tensor(7.5832)\n",
      "890 tensor(8.2567)\n",
      "890 tensor(8.8566)\n",
      "900 tensor(0.6718)\n",
      "900 tensor(1.3984)\n",
      "900 tensor(2.1243)\n",
      "900 tensor(2.7700)\n",
      "900 tensor(3.4420)\n",
      "900 tensor(4.1139)\n",
      "900 tensor(4.7583)\n",
      "900 tensor(5.4298)\n",
      "900 tensor(6.1012)\n",
      "900 tensor(6.8570)\n",
      "900 tensor(7.5836)\n",
      "900 tensor(8.2286)\n",
      "900 tensor(9.0376)\n",
      "910 tensor(0.6718)\n",
      "910 tensor(1.3435)\n",
      "910 tensor(2.0427)\n",
      "910 tensor(2.7419)\n",
      "910 tensor(3.3589)\n",
      "910 tensor(4.0584)\n",
      "910 tensor(4.7859)\n",
      "910 tensor(5.5127)\n",
      "910 tensor(6.1574)\n",
      "910 tensor(6.8567)\n",
      "910 tensor(7.5009)\n",
      "910 tensor(8.2283)\n",
      "910 tensor(9.0376)\n",
      "920 tensor(0.6708)\n",
      "920 tensor(1.3708)\n",
      "920 tensor(2.0707)\n",
      "920 tensor(2.7416)\n",
      "920 tensor(3.4415)\n",
      "920 tensor(4.1124)\n",
      "920 tensor(4.8414)\n",
      "920 tensor(5.4551)\n",
      "920 tensor(6.1552)\n",
      "920 tensor(6.8552)\n",
      "920 tensor(7.5843)\n",
      "920 tensor(8.2553)\n",
      "920 tensor(8.8396)\n",
      "930 tensor(0.6156)\n",
      "930 tensor(1.3153)\n",
      "930 tensor(2.0722)\n",
      "930 tensor(2.7437)\n",
      "930 tensor(3.4151)\n",
      "930 tensor(4.0864)\n",
      "930 tensor(4.7576)\n",
      "930 tensor(5.5143)\n",
      "930 tensor(6.1858)\n",
      "930 tensor(6.9131)\n",
      "930 tensor(7.5024)\n",
      "930 tensor(8.2592)\n",
      "930 tensor(8.8473)\n",
      "940 tensor(0.7210)\n",
      "940 tensor(1.3723)\n",
      "940 tensor(2.0699)\n",
      "940 tensor(2.7206)\n",
      "940 tensor(3.4901)\n",
      "940 tensor(4.1417)\n",
      "940 tensor(4.8392)\n",
      "940 tensor(5.4669)\n",
      "940 tensor(6.1165)\n",
      "940 tensor(6.8391)\n",
      "940 tensor(7.5127)\n",
      "940 tensor(8.2593)\n",
      "940 tensor(8.8625)\n",
      "950 tensor(0.6987)\n",
      "950 tensor(1.3973)\n",
      "950 tensor(2.0959)\n",
      "950 tensor(2.8204)\n",
      "950 tensor(3.4677)\n",
      "950 tensor(4.1402)\n",
      "950 tensor(4.8127)\n",
      "950 tensor(5.5114)\n",
      "950 tensor(6.2100)\n",
      "950 tensor(6.9609)\n",
      "950 tensor(7.6084)\n",
      "950 tensor(8.2552)\n",
      "950 tensor(8.8485)\n",
      "960 tensor(0.6719)\n",
      "960 tensor(1.4257)\n",
      "960 tensor(2.1245)\n",
      "960 tensor(2.7703)\n",
      "960 tensor(3.4423)\n",
      "960 tensor(4.1413)\n",
      "960 tensor(4.8133)\n",
      "960 tensor(5.5395)\n",
      "960 tensor(6.1849)\n",
      "960 tensor(6.8568)\n",
      "960 tensor(7.6107)\n",
      "960 tensor(8.2562)\n",
      "960 tensor(8.8467)\n",
      "970 tensor(0.6998)\n",
      "970 tensor(1.3422)\n",
      "970 tensor(2.1003)\n",
      "970 tensor(2.7999)\n",
      "970 tensor(3.4430)\n",
      "970 tensor(4.1427)\n",
      "970 tensor(4.8709)\n",
      "970 tensor(5.5704)\n",
      "970 tensor(6.2697)\n",
      "970 tensor(6.9691)\n",
      "970 tensor(7.6407)\n",
      "970 tensor(8.2564)\n",
      "970 tensor(8.8415)\n",
      "980 tensor(0.7028)\n",
      "980 tensor(1.4055)\n",
      "980 tensor(2.1081)\n",
      "980 tensor(2.9135)\n",
      "980 tensor(3.5159)\n",
      "980 tensor(4.1843)\n",
      "980 tensor(4.8867)\n",
      "980 tensor(5.6230)\n",
      "980 tensor(6.3585)\n",
      "980 tensor(7.0273)\n",
      "980 tensor(7.5968)\n",
      "980 tensor(8.2650)\n",
      "980 tensor(8.8302)\n",
      "990 tensor(0.6441)\n",
      "990 tensor(1.3155)\n",
      "990 tensor(1.9587)\n",
      "990 tensor(2.6297)\n",
      "990 tensor(3.3295)\n",
      "990 tensor(3.9431)\n",
      "990 tensor(4.7022)\n",
      "990 tensor(5.4019)\n",
      "990 tensor(6.1302)\n",
      "990 tensor(6.8296)\n",
      "990 tensor(7.5290)\n",
      "990 tensor(8.2562)\n",
      "990 tensor(8.8458)\n",
      "1000 tensor(0.6702)\n",
      "1000 tensor(1.3403)\n",
      "1000 tensor(2.0409)\n",
      "1000 tensor(2.6806)\n",
      "1000 tensor(3.3505)\n",
      "1000 tensor(3.9582)\n",
      "1000 tensor(4.7231)\n",
      "1000 tensor(5.4239)\n",
      "1000 tensor(6.1247)\n",
      "1000 tensor(6.9176)\n",
      "1000 tensor(7.5585)\n",
      "1000 tensor(8.2589)\n",
      "1000 tensor(8.8393)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "  total_loss = 0\n",
    "  for train_x, train_y in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion( net(train_x), train_y )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.data\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = net(test_X).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5151515151515151"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.max(test_net, 1)[1]\n",
    "accuracy = sum(test_Y.data.numpy() == result.numpy()) / len(test_Y.data.numpy())\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
